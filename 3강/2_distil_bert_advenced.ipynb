{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbgz49PvHhLt"
      },
      "source": [
        "# (심화과제) Multi-genre natural language inference(MNLI)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1LqgujQUbv6X",
        "outputId": "f8faaad2-ac05-401b-c8f1-45ec07cbb71b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (4.66.5)\n",
            "Requirement already satisfied: boto3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (1.34.154)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (2.32.3)\n",
            "Requirement already satisfied: regex in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (2024.9.11)\n",
            "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.2.0)\n",
            "Requirement already satisfied: sacremoses in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (0.0.43)\n",
            "Requirement already satisfied: datasets in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (2.19.1)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.154 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from boto3) (1.34.154)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from boto3) (0.10.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests) (1.26.19)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: six in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sacremoses) (1.16.0)\n",
            "Requirement already satisfied: click in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: multiprocess in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\n",
            "Requirement already satisfied: aiohttp in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (3.11.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (0.24.6)\n",
            "Requirement already satisfied: packaging in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from botocore<1.35.0,>=1.34.154->boto3) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.2.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (0.2.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (1.18.0)\n",
            "Requirement already satisfied: async-timeout<6.0,>=4.0 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from aiohttp->datasets) (5.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.9/site-packages (from pandas->datasets) (2023.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lGiZUoPby6e",
        "outputId": "592b19ee-1892-4821-fcef-24fe40cc185f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/envs/myenv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Using cache found in /Users/naseunghoo/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset, Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 228,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging as log\n",
        "\n",
        "for handler in log.root.handlers[:]:\n",
        "    log.root.removeHandler(handler)\n",
        "\n",
        "log.basicConfig(\n",
        "    level= log.INFO,\n",
        ")\n",
        "\n",
        "device = 'mps'\n",
        "log.debug(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] nyu-mll/multi_nli 데이터셋 불러오기 \n",
        "\n",
        "- 데이터셋의 구조 파악하기 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli HTTP/1.1\" 200 2731\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): s3.amazonaws.com:443\n",
            "DEBUG:urllib3.connectionpool:https://s3.amazonaws.com:443 \"HEAD /datasets.huggingface.co/datasets/datasets/nyu-mll/multi_nli/nyu-mll/multi_nli.py HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli HTTP/1.1\" 200 2731\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/nyu-mll/multi_nli/resolve/da70db2af9d09693783c3320c4249840212ee221/README.md HTTP/1.1\" 200 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/nyu-mll/multi_nli/resolve/da70db2af9d09693783c3320c4249840212ee221/.huggingface.yaml HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): datasets-server.huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://datasets-server.huggingface.co:443 \"GET /info?dataset=nyu-mll/multi_nli HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli/revision/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 2731\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli/tree/da70db2af9d09693783c3320c4249840212ee221?recursive=False&expand=False HTTP/1.1\" 200 291\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli/tree/da70db2af9d09693783c3320c4249840212ee221/data?recursive=False&expand=False HTTP/1.1\" 200 745\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"GET /api/datasets/nyu-mll/multi_nli/revision/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 2731\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /datasets/nyu-mll/multi_nli/resolve/da70db2af9d09693783c3320c4249840212ee221/dataset_infos.json HTTP/1.1\" 404 0\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"POST /api/datasets/nyu-mll/multi_nli/paths-info/da70db2af9d09693783c3320c4249840212ee221 HTTP/1.1\" 200 235\n",
            "DEBUG:filelock:Attempting to acquire lock 4407410800 on /Users/naseunghoo/.cache/huggingface/datasets/_Users_naseunghoo_.cache_huggingface_datasets_nyu-mll___multi_nli_default_0.0.0_da70db2af9d09693783c3320c4249840212ee221.lock\n",
            "DEBUG:filelock:Lock 4407410800 acquired on /Users/naseunghoo/.cache/huggingface/datasets/_Users_naseunghoo_.cache_huggingface_datasets_nyu-mll___multi_nli_default_0.0.0_da70db2af9d09693783c3320c4249840212ee221.lock\n",
            "DEBUG:fsspec.local:open file: /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221/dataset_info.json\n",
            "DEBUG:filelock:Attempting to release lock 4407410800 on /Users/naseunghoo/.cache/huggingface/datasets/_Users_naseunghoo_.cache_huggingface_datasets_nyu-mll___multi_nli_default_0.0.0_da70db2af9d09693783c3320c4249840212ee221.lock\n",
            "DEBUG:filelock:Lock 4407410800 released on /Users/naseunghoo/.cache/huggingface/datasets/_Users_naseunghoo_.cache_huggingface_datasets_nyu-mll___multi_nli_default_0.0.0_da70db2af9d09693783c3320c4249840212ee221.lock\n",
            "DEBUG:filelock:Attempting to acquire lock 6086801392 on /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221_builder.lock\n",
            "DEBUG:filelock:Lock 6086801392 acquired on /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221_builder.lock\n",
            "DEBUG:fsspec.local:open file: /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221/dataset_info.json\n",
            "DEBUG:filelock:Attempting to release lock 6086801392 on /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221_builder.lock\n",
            "DEBUG:filelock:Lock 6086801392 released on /Users/naseunghoo/.cache/huggingface/datasets/nyu-mll___multi_nli/default/0.0.0/da70db2af9d09693783c3320c4249840212ee221_builder.lock\n"
          ]
        }
      ],
      "source": [
        "ds = load_dataset(\"nyu-mll/multi_nli\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] 데이터셋 구조 확인\n",
        "- ds 는 train, validation_matched, validation_mismatched 의 데이터를 가지고 있습니다.\n",
        "- 레이블 분표율을 확인해본 결과 균등하게 있음을 확인 \n",
        "\n",
        "[FEEDBACK]\n",
        "- 만약 레이블의 데이터가 균등하지 않다면 어떻게 해야할까요?\n",
        "- 균등하지 않는 데이터를 하기 위해서 어떤 작업을 하면 좋나요 ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:root:DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "        num_rows: 392702\n",
            "    })\n",
            "    validation_matched: Dataset({\n",
            "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "        num_rows: 9815\n",
            "    })\n",
            "    validation_mismatched: Dataset({\n",
            "        features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "        num_rows: 9832\n",
            "    })\n",
            "})\n",
            "DEBUG:root:type = <class 'datasets.table.MemoryMappedTable'>\n",
            "DEBUG:root:None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label Distribution: Counter({2: 130903, 1: 130900, 0: 130899})\n",
            "Help on Dataset in module datasets.arrow_dataset object:\n",
            "\n",
            "class Dataset(DatasetInfoMixin, datasets.search.IndexableMixin, TensorflowDatasetMixin)\n",
            " |  Dataset(arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
            " |  \n",
            " |  A Dataset backed by an Arrow table.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Dataset\n",
            " |      DatasetInfoMixin\n",
            " |      datasets.search.IndexableMixin\n",
            " |      TensorflowDatasetMixin\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __del__(self)\n",
            " |  \n",
            " |  __enter__(self)\n",
            " |  \n",
            " |  __exit__(self, exc_type, exc_val, exc_tb)\n",
            " |  \n",
            " |  __getitem__(self, key)\n",
            " |      Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\n",
            " |  \n",
            " |  __getitems__(self, keys: List) -> List\n",
            " |      Can be used to get a batch using a list of integers indices.\n",
            " |  \n",
            " |  __init__(self, arrow_table: datasets.table.Table, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_table: Optional[datasets.table.Table] = None, fingerprint: Optional[str] = None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |      Iterate through the examples.\n",
            " |      \n",
            " |      If a formatting is set with [`Dataset.set_format`] rows will be returned with the\n",
            " |      selected format.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Number of rows in the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.__len__\n",
            " |      <bound method Dataset.__len__ of Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 1066\n",
            " |      })>\n",
            " |      ```\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_column(self, name: str, column: Union[list, <built-in function array>], new_fingerprint: str)\n",
            " |      Add column to Dataset.\n",
            " |      \n",
            " |      <Added version=\"1.7\"/>\n",
            " |      \n",
            " |      Args:\n",
            " |          name (`str`):\n",
            " |              Column name.\n",
            " |          column (`list` or `np.array`):\n",
            " |              Column data to be added.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> more_text = ds[\"text\"]\n",
            " |      >>> ds.add_column(name=\"text_2\", column=more_text)\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label', 'text_2'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  add_elasticsearch_index(self, column: str, index_name: Optional[str] = None, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('elasticsearch.Elasticsearch')] = None, es_index_name: Optional[str] = None, es_index_config: Optional[dict] = None)\n",
            " |      Add a text index using ElasticSearch for fast retrieval. This is done in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`):\n",
            " |              The column of the documents to add to the index.\n",
            " |          index_name (`str`, *optional*):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |              This is the index name that is used to call [`~Dataset.get_nearest_examples`] or [`~Dataset.search`].\n",
            " |              By default it corresponds to `column`.\n",
            " |          host (`str`, *optional*, defaults to `localhost`):\n",
            " |              Host of where ElasticSearch is running.\n",
            " |          port (`str`, *optional*, defaults to `9200`):\n",
            " |              Port of where ElasticSearch is running.\n",
            " |          es_client (`elasticsearch.Elasticsearch`, *optional*):\n",
            " |              The elasticsearch client used to create the index if host and port are `None`.\n",
            " |          es_index_name (`str`, *optional*):\n",
            " |              The elasticsearch index name used to create the index.\n",
            " |          es_index_config (`dict`, *optional*):\n",
            " |              The configuration of the elasticsearch index.\n",
            " |              Default config is:\n",
            " |                  ```\n",
            " |                  {\n",
            " |                      \"settings\": {\n",
            " |                          \"number_of_shards\": 1,\n",
            " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
            " |                      },\n",
            " |                      \"mappings\": {\n",
            " |                          \"properties\": {\n",
            " |                              \"text\": {\n",
            " |                                  \"type\": \"text\",\n",
            " |                                  \"analyzer\": \"standard\",\n",
            " |                                  \"similarity\": \"BM25\"\n",
            " |                              },\n",
            " |                          }\n",
            " |                      },\n",
            " |                  }\n",
            " |                  ```\n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> es_client = elasticsearch.Elasticsearch()\n",
            " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
            " |      >>> ds.add_elasticsearch_index(column='line', es_client=es_client, es_index_name=\"my_es_index\")\n",
            " |      >>> scores, retrieved_examples = ds.get_nearest_examples('line', 'my new query', k=10)\n",
            " |      ```\n",
            " |  \n",
            " |  add_faiss_index(self, column: str, index_name: Optional[str] = None, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
            " |      Add a dense index using Faiss for fast retrieval.\n",
            " |      By default the index is done over the vectors of the specified column.\n",
            " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
            " |      You can find more information about Faiss here:\n",
            " |      \n",
            " |      - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`):\n",
            " |              The column of the vectors to add to the index.\n",
            " |          index_name (`str`, *optional*):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |              This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n",
            " |              By default it corresponds to `column`.\n",
            " |          device (`Union[int, List[int]]`, *optional*):\n",
            " |              If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
            " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
            " |          string_factory (`str`, *optional*):\n",
            " |              This is passed to the index factory of Faiss to create the index.\n",
            " |              Default index class is `IndexFlat`.\n",
            " |          metric_type (`int`, *optional*):\n",
            " |              Type of metric. Ex: `faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n",
            " |          custom_index (`faiss.Index`, *optional*):\n",
            " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
            " |          batch_size (`int`):\n",
            " |              Size of the batch to use while adding vectors to the `FaissIndex`. Default value is `1000`.\n",
            " |              <Added version=\"2.4.0\"/>\n",
            " |          train_size (`int`, *optional*):\n",
            " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
            " |          faiss_verbose (`bool`, defaults to `False`):\n",
            " |              Enable the verbosity of the Faiss index.\n",
            " |          dtype (`data-type`):\n",
            " |              The dtype of the numpy arrays that are indexed.\n",
            " |              Default is `np.float32`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
            " |      >>> ds_with_embeddings = ds.map(lambda example: {'embeddings': embed(example['line']}))\n",
            " |      >>> ds_with_embeddings.add_faiss_index(column='embeddings')\n",
            " |      >>> # query\n",
            " |      >>> scores, retrieved_examples = ds_with_embeddings.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
            " |      >>> # save index\n",
            " |      >>> ds_with_embeddings.save_faiss_index('embeddings', 'my_index.faiss')\n",
            " |      \n",
            " |      >>> ds = datasets.load_dataset('crime_and_punish', split='train')\n",
            " |      >>> # load index\n",
            " |      >>> ds.load_faiss_index('embeddings', 'my_index.faiss')\n",
            " |      >>> # query\n",
            " |      >>> scores, retrieved_examples = ds.get_nearest_examples('embeddings', embed('my new query'), k=10)\n",
            " |      ```\n",
            " |  \n",
            " |  add_faiss_index_from_external_arrays(self, external_arrays: <built-in function array>, index_name: str, device: Optional[int] = None, string_factory: Optional[str] = None, metric_type: Optional[int] = None, custom_index: Optional[ForwardRef('faiss.Index')] = None, batch_size: int = 1000, train_size: Optional[int] = None, faiss_verbose: bool = False, dtype=<class 'numpy.float32'>)\n",
            " |      Add a dense index using Faiss for fast retrieval.\n",
            " |      The index is created using the vectors of `external_arrays`.\n",
            " |      You can specify `device` if you want to run it on GPU (`device` must be the GPU index).\n",
            " |      You can find more information about Faiss here:\n",
            " |      \n",
            " |      - For [string factory](https://github.com/facebookresearch/faiss/wiki/The-index-factory)\n",
            " |      \n",
            " |      Args:\n",
            " |          external_arrays (`np.array`):\n",
            " |              If you want to use arrays from outside the lib for the index, you can set `external_arrays`.\n",
            " |              It will use `external_arrays` to create the Faiss index instead of the arrays in the given `column`.\n",
            " |          index_name (`str`):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |              This is the `index_name` that is used to call [`~datasets.Dataset.get_nearest_examples`] or [`~datasets.Dataset.search`].\n",
            " |          device (Optional `Union[int, List[int]]`, *optional*):\n",
            " |              If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
            " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
            " |          string_factory (`str`, *optional*):\n",
            " |              This is passed to the index factory of Faiss to create the index.\n",
            " |              Default index class is `IndexFlat`.\n",
            " |          metric_type (`int`, *optional*):\n",
            " |              Type of metric. Ex: `faiss.faiss.METRIC_INNER_PRODUCT` or `faiss.METRIC_L2`.\n",
            " |          custom_index (`faiss.Index`, *optional*):\n",
            " |              Custom Faiss index that you already have instantiated and configured for your needs.\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of the batch to use while adding vectors to the FaissIndex. Default value is 1000.\n",
            " |              <Added version=\"2.4.0\"/>\n",
            " |          train_size (`int`, *optional*):\n",
            " |              If the index needs a training step, specifies how many vectors will be used to train the index.\n",
            " |          faiss_verbose (`bool`, defaults to False):\n",
            " |              Enable the verbosity of the Faiss index.\n",
            " |          dtype (`numpy.dtype`):\n",
            " |              The dtype of the numpy arrays that are indexed. Default is np.float32.\n",
            " |  \n",
            " |  add_item(self, item: dict, new_fingerprint: str)\n",
            " |      Add item to Dataset.\n",
            " |      \n",
            " |      <Added version=\"1.7\"/>\n",
            " |      \n",
            " |      Args:\n",
            " |          item (`dict`):\n",
            " |              Item data to be added.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> new_review = {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
            " |      >>> ds = ds.add_item(new_review)\n",
            " |      >>> ds[-1]\n",
            " |      {'label': 0, 'text': 'this movie is the absolute worst thing I have ever seen'}\n",
            " |      ```\n",
            " |  \n",
            " |  align_labels_with_mapping(self, label2id: Dict, label_column: str) -> 'Dataset'\n",
            " |      Align the dataset's label ID and label name mapping to match an input `label2id` mapping.\n",
            " |      This is useful when you want to ensure that a model's predicted labels are aligned with the dataset.\n",
            " |      The alignment in done using the lowercase label names.\n",
            " |      \n",
            " |      Args:\n",
            " |          label2id (`dict`):\n",
            " |              The label name to ID mapping to align the dataset with.\n",
            " |          label_column (`str`):\n",
            " |              The column name of labels to align on.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> # dataset with mapping {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n",
            " |      >>> ds = load_dataset(\"glue\", \"mnli\", split=\"train\")\n",
            " |      >>> # mapping to align with\n",
            " |      >>> label2id = {'CONTRADICTION': 0, 'NEUTRAL': 1, 'ENTAILMENT': 2}\n",
            " |      >>> ds_aligned = ds.align_labels_with_mapping(label2id, \"label\")\n",
            " |      ```\n",
            " |  \n",
            " |  cast(self, features: datasets.features.features.Features, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, num_proc: Optional[int] = None) -> 'Dataset'\n",
            " |      Cast the dataset to a new set of features.\n",
            " |      \n",
            " |      Args:\n",
            " |          features ([`Features`]):\n",
            " |              New features to cast the dataset to.\n",
            " |              The name of the fields in the features must match the current column names.\n",
            " |              The type of the data must also be convertible from one type to the other.\n",
            " |              For non-trivial conversion, e.g. `str` <-> `ClassLabel` you should use [`~datasets.Dataset.map`] to update the Dataset.\n",
            " |          batch_size (`int`, defaults to `1000`):\n",
            " |              Number of examples per batch provided to cast.\n",
            " |              If `batch_size <= 0` or `batch_size == None` then provide the full dataset as a single batch to cast.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          load_from_cache_file (`bool`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          cache_file_name (`str`, *optional*, defaults to `None`):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running [`~datasets.Dataset.map`].\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset with casted features.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset, ClassLabel, Value\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      >>> new_features = ds.features.copy()\n",
            " |      >>> new_features['label'] = ClassLabel(names=['bad', 'good'])\n",
            " |      >>> new_features['text'] = Value('large_string')\n",
            " |      >>> ds = ds.cast(new_features)\n",
            " |      >>> ds.features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
            " |       'text': Value(dtype='large_string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  cast_column(self, column: str, feature: Union[dict, list, tuple, datasets.features.features.Value, datasets.features.features.ClassLabel, datasets.features.translation.Translation, datasets.features.translation.TranslationVariableLanguages, datasets.features.features.Sequence, datasets.features.features.Array2D, datasets.features.features.Array3D, datasets.features.features.Array4D, datasets.features.features.Array5D, datasets.features.audio.Audio, datasets.features.image.Image], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Cast column to feature for decoding.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`):\n",
            " |              Column name.\n",
            " |          feature (`FeatureType`):\n",
            " |              Target feature.\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      >>> ds = ds.cast_column('label', ClassLabel(names=['bad', 'good']))\n",
            " |      >>> ds.features\n",
            " |      {'label': ClassLabel(num_classes=2, names=['bad', 'good'], id=None),\n",
            " |       'text': Value(dtype='string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  class_encode_column(self, column: str, include_nulls: bool = False) -> 'Dataset'\n",
            " |      Casts the given column as [`~datasets.features.ClassLabel`] and updates the table.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`):\n",
            " |              The name of the column to cast (list all the column names with [`~datasets.Dataset.column_names`])\n",
            " |          include_nulls (`bool`, defaults to `False`):\n",
            " |              Whether to include null values in the class labels. If `True`, the null values will be encoded as the `\"None\"` class label.\n",
            " |      \n",
            " |              <Added version=\"1.14.2\"/>\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"boolq\", split=\"validation\")\n",
            " |      >>> ds.features\n",
            " |      {'answer': Value(dtype='bool', id=None),\n",
            " |       'passage': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None)}\n",
            " |      >>> ds = ds.class_encode_column('answer')\n",
            " |      >>> ds.features\n",
            " |      {'answer': ClassLabel(num_classes=2, names=['False', 'True'], id=None),\n",
            " |       'passage': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None)}\n",
            " |      ```\n",
            " |  \n",
            " |  cleanup_cache_files(self) -> int\n",
            " |      Clean up all cache files in the dataset cache directory, excepted the currently used cache file if there is\n",
            " |      one.\n",
            " |      \n",
            " |      Be careful when running this command that no other process is currently using other cache files.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: Number of removed files.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.cleanup_cache_files()\n",
            " |      10\n",
            " |      ```\n",
            " |  \n",
            " |  export(self, filename: str, format: str = 'tfrecord')\n",
            " |      Writes the Arrow dataset to a TFRecord file.\n",
            " |      \n",
            " |      The dataset must already be in tensorflow format. The records will be written with\n",
            " |      keys from `dataset._format_columns`.\n",
            " |      \n",
            " |      Args:\n",
            " |          filename (`str`): The filename, including the `.tfrecord` extension, to write to.\n",
            " |          format (`str`, optional, default `\"tfrecord\"`): The type of output file. Currently this is a no-op, as\n",
            " |              TFRecords are the only option. This enables a more flexible function signature later.\n",
            " |  \n",
            " |  filter(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
            " |      Apply a filter function to all the elements in the table in batches\n",
            " |      and update the table so that the dataset only includes examples according to the filter function.\n",
            " |      \n",
            " |      Args:\n",
            " |          function (`Callable`): Callable with one of the following signatures:\n",
            " |      \n",
            " |              - `function(example: Dict[str, Any]) -> bool` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
            " |              - `function(example: Dict[str, Any], *extra_args) -> bool` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
            " |              - `function(batch: Dict[str, List]) -> List[bool]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
            " |              - `function(batch: Dict[str, List], *extra_args) -> List[bool]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
            " |      \n",
            " |              If no function is provided, defaults to an always `True` function: `lambda x: True`.\n",
            " |          with_indices (`bool`, defaults to `False`):\n",
            " |              Provide example indices to `function`. Note that in this case the\n",
            " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
            " |          with_rank (`bool`, defaults to `False`):\n",
            " |              Provide process rank to `function`. Note that in this case the\n",
            " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
            " |          input_columns (`str` or `List[str]`, *optional*):\n",
            " |              The columns to be passed into `function` as\n",
            " |              positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n",
            " |          batched (`bool`, defaults to `False`):\n",
            " |              Provide batch of examples to `function`.\n",
            " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
            " |              Number of examples per batch provided to `function` if\n",
            " |              `batched = True`. If `batched = False`, one example per batch is passed to `function`.\n",
            " |              If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          cache_file_name (`str`, *optional*):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          fn_kwargs (`dict`, *optional*):\n",
            " |              Keyword arguments to be passed to `function`.\n",
            " |          num_proc (`int`, *optional*):\n",
            " |              Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing.\n",
            " |          suffix_template (`str`):\n",
            " |              If `cache_file_name` is specified, then this suffix will be added at the end of the base name of each.\n",
            " |              For example, if `cache_file_name` is `\"processed.arrow\"`, then for `rank = 1` and `num_proc = 4`,\n",
            " |              the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix (default\n",
            " |              `_{rank:05d}_of_{num_proc:05d}`).\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |          desc (`str`, *optional*, defaults to `None`):\n",
            " |              Meaningful description to be displayed alongside with the progress bar while filtering examples.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.filter(lambda x: x[\"label\"] == 1)\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 533\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  flatten(self, new_fingerprint: Optional[str] = None, max_depth=16) -> 'Dataset'\n",
            " |      Flatten the table.\n",
            " |      Each column with a struct type is flattened into one column per struct field.\n",
            " |      Other columns are left unchanged.\n",
            " |      \n",
            " |      Args:\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset with flattened columns.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"squad\", split=\"train\")\n",
            " |      >>> ds.features\n",
            " |      {'answers': Sequence(feature={'text': Value(dtype='string', id=None), 'answer_start': Value(dtype='int32', id=None)}, length=-1, id=None),\n",
            " |       'context': Value(dtype='string', id=None),\n",
            " |       'id': Value(dtype='string', id=None),\n",
            " |       'question': Value(dtype='string', id=None),\n",
            " |       'title': Value(dtype='string', id=None)}\n",
            " |      >>> ds.flatten()\n",
            " |      Dataset({\n",
            " |          features: ['id', 'title', 'context', 'question', 'answers.text', 'answers.answer_start'],\n",
            " |          num_rows: 87599\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  flatten_indices(self, keep_in_memory: bool = False, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, num_proc: Optional[int] = None, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Create and cache a new Dataset by flattening the indices mapping.\n",
            " |      \n",
            " |      Args:\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          cache_file_name (`str`, *optional*, default `None`):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          features (`Optional[datasets.Features]`, defaults to `None`):\n",
            " |              Use a specific [`Features`] to store the cache file\n",
            " |              instead of the automatically generated one.\n",
            " |          disable_nullable (`bool`, defaults to `False`):\n",
            " |              Allow null values in the table.\n",
            " |          num_proc (`int`, optional, default `None`):\n",
            " |              Max number of processes when generating cache. Already cached shards are loaded sequentially\n",
            " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
            " |  \n",
            " |  formatted_as(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
            " |      To be used in a `with` statement. Set `__getitem__` return format (type and columns).\n",
            " |      \n",
            " |      Args:\n",
            " |          type (`str`, *optional*):\n",
            " |              Output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
            " |              `None` means `__getitem__`` returns python objects (default).\n",
            " |          columns (`List[str]`, *optional*):\n",
            " |              Columns to format in the output.\n",
            " |              `None` means `__getitem__` returns all columns (default).\n",
            " |          output_all_columns (`bool`, defaults to `False`):\n",
            " |              Keep un-formatted columns as well in the output (as python objects).\n",
            " |          **format_kwargs (additional keyword arguments):\n",
            " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |  \n",
            " |  iter(self, batch_size: int, drop_last_batch: bool = False)\n",
            " |      Iterate through the batches of size `batch_size`.\n",
            " |      \n",
            " |      If a formatting is set with [`~datasets.Dataset.set_format`] rows will be returned with the\n",
            " |      selected format.\n",
            " |      \n",
            " |      Args:\n",
            " |          batch_size (:obj:`int`): size of each batch to yield.\n",
            " |          drop_last_batch (:obj:`bool`, default `False`): Whether a last batch smaller than the batch_size should be\n",
            " |              dropped\n",
            " |  \n",
            " |  map(self, function: Optional[Callable] = None, with_indices: bool = False, with_rank: bool = False, input_columns: Union[str, List[str], NoneType] = None, batched: bool = False, batch_size: Optional[int] = 1000, drop_last_batch: bool = False, remove_columns: Union[str, List[str], NoneType] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, features: Optional[datasets.features.features.Features] = None, disable_nullable: bool = False, fn_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, suffix_template: str = '_{rank:05d}_of_{num_proc:05d}', new_fingerprint: Optional[str] = None, desc: Optional[str] = None) -> 'Dataset'\n",
            " |      Apply a function to all the examples in the table (individually or in batches) and update the table.\n",
            " |      If your function returns a column that already exists, then it overwrites it.\n",
            " |      \n",
            " |      You can specify whether the function should be batched or not with the `batched` parameter:\n",
            " |      \n",
            " |      - If batched is `False`, then the function takes 1 example in and should return 1 example.\n",
            " |        An example is a dictionary, e.g. `{\"text\": \"Hello there !\"}`.\n",
            " |      - If batched is `True` and `batch_size` is 1, then the function takes a batch of 1 example as input and can return a batch with 1 or more examples.\n",
            " |        A batch is a dictionary, e.g. a batch of 1 example is `{\"text\": [\"Hello there !\"]}`.\n",
            " |      - If batched is `True` and `batch_size` is `n > 1`, then the function takes a batch of `n` examples as input and can return a batch with `n` examples, or with an arbitrary number of examples.\n",
            " |        Note that the last batch may have less than `n` examples.\n",
            " |        A batch is a dictionary, e.g. a batch of `n` examples is `{\"text\": [\"Hello there !\"] * n}`.\n",
            " |      \n",
            " |      Args:\n",
            " |          function (`Callable`): Function with one of the following signatures:\n",
            " |      \n",
            " |              - `function(example: Dict[str, Any]) -> Dict[str, Any]` if `batched=False` and `with_indices=False` and `with_rank=False`\n",
            " |              - `function(example: Dict[str, Any], *extra_args) -> Dict[str, Any]` if `batched=False` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
            " |              - `function(batch: Dict[str, List]) -> Dict[str, List]` if `batched=True` and `with_indices=False` and `with_rank=False`\n",
            " |              - `function(batch: Dict[str, List], *extra_args) -> Dict[str, List]` if `batched=True` and `with_indices=True` and/or `with_rank=True` (one extra arg for each)\n",
            " |      \n",
            " |              For advanced usage, the function can also return a `pyarrow.Table`.\n",
            " |              Moreover if your function returns nothing (`None`), then `map` will run your function and return the dataset unchanged.\n",
            " |              If no function is provided, default to identity function: `lambda x: x`.\n",
            " |          with_indices (`bool`, defaults to `False`):\n",
            " |              Provide example indices to `function`. Note that in this case the\n",
            " |              signature of `function` should be `def function(example, idx[, rank]): ...`.\n",
            " |          with_rank (`bool`, defaults to `False`):\n",
            " |              Provide process rank to `function`. Note that in this case the\n",
            " |              signature of `function` should be `def function(example[, idx], rank): ...`.\n",
            " |          input_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
            " |              The columns to be passed into `function`\n",
            " |              as positional arguments. If `None`, a `dict` mapping to all formatted columns is passed as one argument.\n",
            " |          batched (`bool`, defaults to `False`):\n",
            " |              Provide batch of examples to `function`.\n",
            " |          batch_size (`int`, *optional*, defaults to `1000`):\n",
            " |              Number of examples per batch provided to `function` if `batched=True`.\n",
            " |              If `batch_size <= 0` or `batch_size == None`, provide the full dataset as a single batch to `function`.\n",
            " |          drop_last_batch (`bool`, defaults to `False`):\n",
            " |              Whether a last batch smaller than the batch_size should be\n",
            " |              dropped instead of being processed by the function.\n",
            " |          remove_columns (`Optional[Union[str, List[str]]]`, defaults to `None`):\n",
            " |              Remove a selection of columns while doing the mapping.\n",
            " |              Columns will be removed before updating the examples with the output of `function`, i.e. if `function` is adding\n",
            " |              columns with names in `remove_columns`, these columns will be kept.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the current computation from `function`\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          cache_file_name (`str`, *optional*, defaults to `None`):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              results of the computation instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          features (`Optional[datasets.Features]`, defaults to `None`):\n",
            " |              Use a specific Features to store the cache file\n",
            " |              instead of the automatically generated one.\n",
            " |          disable_nullable (`bool`, defaults to `False`):\n",
            " |              Disallow null values in the table.\n",
            " |          fn_kwargs (`Dict`, *optional*, defaults to `None`):\n",
            " |              Keyword arguments to be passed to `function`.\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Max number of processes when generating cache. Already cached shards are loaded sequentially.\n",
            " |          suffix_template (`str`):\n",
            " |              If `cache_file_name` is specified, then this suffix\n",
            " |              will be added at the end of the base name of each. Defaults to `\"_{rank:05d}_of_{num_proc:05d}\"`. For example, if `cache_file_name` is \"processed.arrow\", then for\n",
            " |              `rank=1` and `num_proc=4`, the resulting file would be `\"processed_00001_of_00004.arrow\"` for the default suffix.\n",
            " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |          desc (`str`, *optional*, defaults to `None`):\n",
            " |              Meaningful description to be displayed alongside with the progress bar while mapping examples.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> def add_prefix(example):\n",
            " |      ...     example[\"text\"] = \"Review: \" + example[\"text\"]\n",
            " |      ...     return example\n",
            " |      >>> ds = ds.map(add_prefix)\n",
            " |      >>> ds[0:3][\"text\"]\n",
            " |      ['Review: compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .',\n",
            " |       'Review: the soundtrack alone is worth the price of admission .',\n",
            " |       'Review: rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .']\n",
            " |      \n",
            " |      # process a batch of examples\n",
            " |      >>> ds = ds.map(lambda example: tokenizer(example[\"text\"]), batched=True)\n",
            " |      # set number of processors\n",
            " |      >>> ds = ds.map(add_prefix, num_proc=4)\n",
            " |      ```\n",
            " |  \n",
            " |  prepare_for_task(self, task: Union[str, datasets.tasks.base.TaskTemplate], id: int = 0) -> 'Dataset'\n",
            " |      Prepare a dataset for the given task by casting the dataset's [`Features`] to standardized column names and types as detailed in [`datasets.tasks`](./task_templates).\n",
            " |      \n",
            " |      Casts [`datasets.DatasetInfo.features`] according to a task-specific schema. Intended for single-use only, so all task templates are removed from [`datasets.DatasetInfo.task_templates`] after casting.\n",
            " |      \n",
            " |      Args:\n",
            " |          task (`Union[str, TaskTemplate]`):\n",
            " |              The task to prepare the dataset for during training and evaluation. If `str`, supported tasks include:\n",
            " |      \n",
            " |              - `\"text-classification\"`\n",
            " |              - `\"question-answering\"`\n",
            " |      \n",
            " |              If [`TaskTemplate`], must be one of the task templates in [`datasets.tasks`](./task_templates).\n",
            " |          id (`int`, defaults to `0`):\n",
            " |              The id required to unambiguously identify the task template when multiple task templates of the same type are supported.\n",
            " |  \n",
            " |  push_to_hub(self, repo_id: str, config_name: str = 'default', set_default: Optional[bool] = None, split: Optional[str] = None, data_dir: Optional[str] = None, commit_message: Optional[str] = None, commit_description: Optional[str] = None, private: Optional[bool] = False, token: Optional[str] = None, revision: Optional[str] = None, branch='deprecated', create_pr: Optional[bool] = False, max_shard_size: Union[str, int, NoneType] = None, num_shards: Optional[int] = None, embed_external_files: bool = True) -> huggingface_hub.hf_api.CommitInfo\n",
            " |      Pushes the dataset to the hub as a Parquet dataset.\n",
            " |      The dataset is pushed using HTTP requests and does not need to have neither git or git-lfs installed.\n",
            " |      \n",
            " |      The resulting Parquet files are self-contained by default. If your dataset contains [`Image`] or [`Audio`]\n",
            " |      data, the Parquet files will store the bytes of your images or audio files.\n",
            " |      You can disable this by setting `embed_external_files` to `False`.\n",
            " |      \n",
            " |      Args:\n",
            " |          repo_id (`str`):\n",
            " |              The ID of the repository to push to in the following format: `<user>/<dataset_name>` or\n",
            " |              `<org>/<dataset_name>`. Also accepts `<dataset_name>`, which will default to the namespace\n",
            " |              of the logged-in user.\n",
            " |          config_name (`str`, defaults to \"default\"):\n",
            " |              The configuration name (or subset) of a dataset. Defaults to \"default\".\n",
            " |          set_default (`bool`, *optional*):\n",
            " |              Whether to set this configuration as the default one. Otherwise, the default configuration is the one\n",
            " |              named \"default\".\n",
            " |          split (`str`, *optional*):\n",
            " |              The name of the split that will be given to that dataset. Defaults to `self.split`.\n",
            " |          data_dir (`str`, *optional*):\n",
            " |              Directory name that will contain the uploaded data files. Defaults to the `config_name` if different\n",
            " |              from \"default\", else \"data\".\n",
            " |      \n",
            " |              <Added version=\"2.17.0\"/>\n",
            " |          commit_message (`str`, *optional*):\n",
            " |              Message to commit while pushing. Will default to `\"Upload dataset\"`.\n",
            " |          commit_description (`str`, *optional*):\n",
            " |              Description of the commit that will be created.\n",
            " |              Additionally, description of the PR if a PR is created (`create_pr` is True).\n",
            " |      \n",
            " |              <Added version=\"2.16.0\"/>\n",
            " |          private (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether the dataset repository should be set to private or not. Only affects repository creation:\n",
            " |              a repository that already exists will not be affected by that parameter.\n",
            " |          token (`str`, *optional*):\n",
            " |              An optional authentication token for the Hugging Face Hub. If no token is passed, will default\n",
            " |              to the token saved locally when logging in with `huggingface-cli login`. Will raise an error\n",
            " |              if no token is passed and the user is not logged-in.\n",
            " |          revision (`str`, *optional*):\n",
            " |              Branch to push the uploaded files to. Defaults to the `\"main\"` branch.\n",
            " |      \n",
            " |              <Added version=\"2.15.0\"/>\n",
            " |          branch (`str`, *optional*):\n",
            " |              The git branch on which to push the dataset. This defaults to the default branch as specified\n",
            " |              in your repository, which defaults to `\"main\"`.\n",
            " |      \n",
            " |              <Deprecated version=\"2.15.0\">\n",
            " |      \n",
            " |              `branch` was deprecated in favor of `revision` in version 2.15.0 and will be removed in 3.0.0.\n",
            " |      \n",
            " |              </Deprecated>\n",
            " |          create_pr (`bool`, *optional*, defaults to `False`):\n",
            " |              Whether to create a PR with the uploaded files or directly commit.\n",
            " |      \n",
            " |              <Added version=\"2.15.0\"/>\n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
            " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by\n",
            " |              a unit (like `\"5MB\"`).\n",
            " |          num_shards (`int`, *optional*):\n",
            " |              Number of shards to write. By default, the number of shards depends on `max_shard_size`.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          embed_external_files (`bool`, defaults to `True`):\n",
            " |              Whether to embed file bytes in the shards.\n",
            " |              In particular, this will do the following before the push for the fields of type:\n",
            " |      \n",
            " |              - [`Audio`] and [`Image`]: remove local path information and embed file content in the Parquet files.\n",
            " |      \n",
            " |      Return:\n",
            " |          huggingface_hub.CommitInfo\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\")\n",
            " |      >>> dataset_dict.push_to_hub(\"<organization>/<dataset_id>\", private=True)\n",
            " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\", max_shard_size=\"1GB\")\n",
            " |      >>> dataset.push_to_hub(\"<organization>/<dataset_id>\", num_shards=1024)\n",
            " |      ```\n",
            " |      \n",
            " |      If your dataset has multiple splits (e.g. train/validation/test):\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> train_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"train\")\n",
            " |      >>> val_dataset.push_to_hub(\"<organization>/<dataset_id>\", split=\"validation\")\n",
            " |      >>> # later\n",
            " |      >>> dataset = load_dataset(\"<organization>/<dataset_id>\")\n",
            " |      >>> train_dataset = dataset[\"train\"]\n",
            " |      >>> val_dataset = dataset[\"validation\"]\n",
            " |      ```\n",
            " |      \n",
            " |      If you want to add a new configuration (or subset) to a dataset (e.g. if the dataset has multiple tasks/versions/languages):\n",
            " |      \n",
            " |      ```python\n",
            " |      >>> english_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"en\")\n",
            " |      >>> french_dataset.push_to_hub(\"<organization>/<dataset_id>\", \"fr\")\n",
            " |      >>> # later\n",
            " |      >>> english_dataset = load_dataset(\"<organization>/<dataset_id>\", \"en\")\n",
            " |      >>> french_dataset = load_dataset(\"<organization>/<dataset_id>\", \"fr\")\n",
            " |      ```\n",
            " |  \n",
            " |  remove_columns(self, column_names: Union[str, List[str]], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Remove one or several column(s) in the dataset and the features associated to them.\n",
            " |      \n",
            " |      You can also remove a column using [`~datasets.Dataset.map`] with `remove_columns` but the present method\n",
            " |      doesn't copy the data of the remaining columns and is thus faster.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_names (`Union[str, List[str]]`):\n",
            " |              Name of the column(s) to remove.\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset object without the columns to remove.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds = ds.remove_columns('label')\n",
            " |      Dataset({\n",
            " |          features: ['text'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      >>> ds = ds.remove_columns(column_names=ds.column_names) # Removing all the columns returns an empty dataset with the `num_rows` property set to 0\n",
            " |      Dataset({\n",
            " |          features: [],\n",
            " |          num_rows: 0\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  rename_column(self, original_column_name: str, new_column_name: str, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Rename a column in the dataset, and move the features associated to the original column under the new column\n",
            " |      name.\n",
            " |      \n",
            " |      Args:\n",
            " |          original_column_name (`str`):\n",
            " |              Name of the column to rename.\n",
            " |          new_column_name (`str`):\n",
            " |              New name for the column.\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset with a renamed column.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds = ds.rename_column('label', 'label_new')\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label_new'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  rename_columns(self, column_mapping: Dict[str, str], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Rename several columns in the dataset, and move the features associated to the original columns under\n",
            " |      the new column names.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_mapping (`Dict[str, str]`):\n",
            " |              A mapping of columns to rename to their new names\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset with renamed columns\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds = ds.rename_columns({'text': 'text_new', 'label': 'label_new'})\n",
            " |      Dataset({\n",
            " |          features: ['text_new', 'label_new'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  reset_format(self)\n",
            " |      Reset `__getitem__` return format to python objects and all columns.\n",
            " |      \n",
            " |      Same as `self.set_format()`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds.set_format(type='numpy', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
            " |      >>> ds.format\n",
            " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': 'numpy'}\n",
            " |      >>> ds.reset_format()\n",
            " |      >>> ds.format\n",
            " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': None}\n",
            " |      ```\n",
            " |  \n",
            " |  save_to_disk(self, dataset_path: Union[str, bytes, os.PathLike], fs='deprecated', max_shard_size: Union[str, int, NoneType] = None, num_shards: Optional[int] = None, num_proc: Optional[int] = None, storage_options: Optional[dict] = None)\n",
            " |      Saves a dataset to a dataset directory, or in a filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n",
            " |      \n",
            " |      For [`Image`] and [`Audio`] data:\n",
            " |      \n",
            " |      All the Image() and Audio() data are stored in the arrow files.\n",
            " |      If you want to store paths or urls, please use the Value(\"string\") type.\n",
            " |      \n",
            " |      Args:\n",
            " |          dataset_path (`str`):\n",
            " |              Path (e.g. `dataset/train`) or remote URI (e.g. `s3://my-bucket/dataset/train`)\n",
            " |              of the dataset directory where the dataset will be saved to.\n",
            " |          fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n",
            " |              Instance of the remote filesystem where the dataset will be saved to.\n",
            " |      \n",
            " |              <Deprecated version=\"2.8.0\">\n",
            " |      \n",
            " |              `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n",
            " |              Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n",
            " |      \n",
            " |              </Deprecated>\n",
            " |      \n",
            " |          max_shard_size (`int` or `str`, *optional*, defaults to `\"500MB\"`):\n",
            " |              The maximum size of the dataset shards to be uploaded to the hub. If expressed as a string, needs to be digits followed by a unit\n",
            " |              (like `\"50MB\"`).\n",
            " |          num_shards (`int`, *optional*):\n",
            " |              Number of shards to write. By default the number of shards depends on `max_shard_size` and `num_proc`.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          num_proc (`int`, *optional*):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              Multiprocessing is disabled by default.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.save_to_disk(\"path/to/dataset/directory\")\n",
            " |      >>> ds.save_to_disk(\"path/to/dataset/directory\", max_shard_size=\"1GB\")\n",
            " |      >>> ds.save_to_disk(\"path/to/dataset/directory\", num_shards=1024)\n",
            " |      ```\n",
            " |  \n",
            " |  select(self, indices: Iterable, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Create a new dataset with rows selected following the list/array of indices.\n",
            " |      \n",
            " |      Args:\n",
            " |          indices (`range`, `list`, `iterable`, `ndarray` or `Series`):\n",
            " |              Range, list or 1D-array of integer indices for indexing.\n",
            " |              If the indices correspond to a contiguous range, the Arrow table is simply sliced.\n",
            " |              However passing a list of indices that are not contiguous creates indices mapping, which is much less efficient,\n",
            " |              but still faster than recreating an Arrow table made of the requested rows.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the indices mapping in memory instead of writing it to a cache file.\n",
            " |          indices_cache_file_name (`str`, *optional*, defaults to `None`):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              indices mapping instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.select(range(4))\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 4\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  select_columns(self, column_names: Union[str, List[str]], new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Select one or several column(s) in the dataset and the features\n",
            " |      associated to them.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_names (`Union[str, List[str]]`):\n",
            " |              Name of the column(s) to keep.\n",
            " |          new_fingerprint (`str`, *optional*):\n",
            " |              The new fingerprint of the dataset after transform. If `None`,\n",
            " |              the new fingerprint is computed using a hash of the previous\n",
            " |              fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]: A copy of the dataset object which only consists of\n",
            " |          selected columns.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.select_columns(['text'])\n",
            " |      Dataset({\n",
            " |          features: ['text'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  set_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
            " |      Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
            " |      The format `type` (for example \"numpy\") is used to format batches when using `__getitem__`.\n",
            " |      It's also possible to use custom transforms for formatting using [`~datasets.Dataset.set_transform`].\n",
            " |      \n",
            " |      Args:\n",
            " |          type (`str`, *optional*):\n",
            " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
            " |              `None` means `__getitem__` returns python objects (default).\n",
            " |          columns (`List[str]`, *optional*):\n",
            " |              Columns to format in the output.\n",
            " |              `None` means `__getitem__` returns all columns (default).\n",
            " |          output_all_columns (`bool`, defaults to `False`):\n",
            " |              Keep un-formatted columns as well in the output (as python objects).\n",
            " |          **format_kwargs (additional keyword arguments):\n",
            " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |      \n",
            " |      It is possible to call [`~datasets.Dataset.map`] after calling `set_format`. Since `map` may add new columns, then the list of formatted columns\n",
            " |      gets updated. In this case, if you apply `map` on a dataset to add a new column, then this column will be formatted as:\n",
            " |      \n",
            " |          ```\n",
            " |          new formatted columns = (all columns - previously unformatted columns)\n",
            " |          ```\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds.set_format(type='numpy', columns=['text', 'label'])\n",
            " |      >>> ds.format\n",
            " |      {'type': 'numpy',\n",
            " |      'format_kwargs': {},\n",
            " |      'columns': ['text', 'label'],\n",
            " |      'output_all_columns': False}\n",
            " |      ```\n",
            " |  \n",
            " |  set_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
            " |      Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n",
            " |      As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n",
            " |      \n",
            " |      Args:\n",
            " |          transform (`Callable`, *optional*):\n",
            " |              User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n",
            " |              A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n",
            " |              This function is applied right before returning the objects in `__getitem__`.\n",
            " |          columns (`List[str]`, *optional*):\n",
            " |              Columns to format in the output.\n",
            " |              If specified, then the input batch of the transform only contains those columns.\n",
            " |          output_all_columns (`bool`, defaults to `False`):\n",
            " |              Keep un-formatted columns as well in the output (as python objects).\n",
            " |              If set to True, then the other un-formatted columns are kept with the output of the transform.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
            " |      >>> def encode(batch):\n",
            " |      ...     return tokenizer(batch['text'], padding=True, truncation=True, return_tensors='pt')\n",
            " |      >>> ds.set_transform(encode)\n",
            " |      >>> ds[0]\n",
            " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            " |       1, 1]),\n",
            " |       'input_ids': tensor([  101, 29353,  2135, 15102,  1996,  9428, 20868,  2890,  8663,  6895,\n",
            " |               20470,  2571,  3663,  2090,  4603,  3017,  3008,  1998,  2037, 24211,\n",
            " |               5637,  1998, 11690,  2336,  1012,   102]),\n",
            " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            " |               0, 0])}\n",
            " |      ```\n",
            " |  \n",
            " |  shard(self, num_shards: int, index: int, contiguous: bool = False, keep_in_memory: bool = False, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000) -> 'Dataset'\n",
            " |      Return the `index`-nth shard from dataset split into `num_shards` pieces.\n",
            " |      \n",
            " |      This shards deterministically. `dset.shard(n, i)` will contain all elements of dset whose\n",
            " |      index mod `n = i`.\n",
            " |      \n",
            " |      `dset.shard(n, i, contiguous=True)` will instead split dset into contiguous chunks,\n",
            " |      so it can be easily concatenated back together after processing. If `n % i == l`, then the\n",
            " |      first `l` shards will have length `(n // i) + 1`, and the remaining shards will have length `(n // i)`.\n",
            " |      `datasets.concatenate([dset.shard(n, i, contiguous=True) for i in range(n)])` will return\n",
            " |      a dataset with the same order as the original.\n",
            " |      \n",
            " |      Be sure to shard before using any randomizing operator (such as `shuffle`).\n",
            " |      It is best if the shard operator is used early in the dataset pipeline.\n",
            " |      \n",
            " |      \n",
            " |      Args:\n",
            " |          num_shards (`int`):\n",
            " |              How many shards to split the dataset into.\n",
            " |          index (`int`):\n",
            " |              Which shard to select and return.\n",
            " |          contiguous: (`bool`, defaults to `False`):\n",
            " |              Whether to select contiguous blocks of indices for shards.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the dataset in memory instead of writing it to a cache file.\n",
            " |          indices_cache_file_name (`str`, *optional*):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              indices of each shard instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 1066\n",
            " |      })\n",
            " |      >>> ds.shard(num_shards=2, index=0)\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 533\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  shuffle(self, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Create a new Dataset where the rows are shuffled.\n",
            " |      \n",
            " |      Currently shuffling uses numpy random generators.\n",
            " |      You can either supply a NumPy BitGenerator to use, or a seed to initiate NumPy's default random generator (PCG64).\n",
            " |      \n",
            " |      Shuffling takes the list of indices `[0:len(my_dataset)]` and shuffles it to create an indices mapping.\n",
            " |      However as soon as your [`Dataset`] has an indices mapping, the speed can become 10x slower.\n",
            " |      This is because there is an extra step to get the row index to read using the indices mapping, and most importantly, you aren't reading contiguous chunks of data anymore.\n",
            " |      To restore the speed, you'd need to rewrite the entire dataset on your disk again using [`Dataset.flatten_indices`], which removes the indices mapping.\n",
            " |      This may take a lot of time depending of the size of your dataset though:\n",
            " |      \n",
            " |      ```python\n",
            " |      my_dataset[0]  # fast\n",
            " |      my_dataset = my_dataset.shuffle(seed=42)\n",
            " |      my_dataset[0]  # up to 10x slower\n",
            " |      my_dataset = my_dataset.flatten_indices()  # rewrite the shuffled dataset on disk as contiguous chunks of data\n",
            " |      my_dataset[0]  # fast again\n",
            " |      ```\n",
            " |      \n",
            " |      In this case, we recommend switching to an [`IterableDataset`] and leveraging its fast approximate shuffling method [`IterableDataset.shuffle`].\n",
            " |      It only shuffles the shards order and adds a shuffle buffer to your dataset, which keeps the speed of your dataset optimal:\n",
            " |      \n",
            " |      ```python\n",
            " |      my_iterable_dataset = my_dataset.to_iterable_dataset(num_shards=128)\n",
            " |      for example in enumerate(my_iterable_dataset):  # fast\n",
            " |          pass\n",
            " |      \n",
            " |      shuffled_iterable_dataset = my_iterable_dataset.shuffle(seed=42, buffer_size=100)\n",
            " |      \n",
            " |      for example in enumerate(shuffled_iterable_dataset):  # as fast as before\n",
            " |          pass\n",
            " |      ```\n",
            " |      \n",
            " |      Args:\n",
            " |          seed (`int`, *optional*):\n",
            " |              A seed to initialize the default BitGenerator if `generator=None`.\n",
            " |              If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n",
            " |              If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
            " |          generator (`numpy.random.Generator`, *optional*):\n",
            " |              Numpy random Generator to use to compute the permutation of the dataset rows.\n",
            " |              If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n",
            " |          keep_in_memory (`bool`, default `False`):\n",
            " |              Keep the shuffled indices in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the shuffled indices\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          indices_cache_file_name (`str`, *optional*):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              shuffled indices instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds['label'][:10]\n",
            " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " |      \n",
            " |      # set a seed\n",
            " |      >>> shuffled_ds = ds.shuffle(seed=42)\n",
            " |      >>> shuffled_ds['label'][:10]\n",
            " |      [1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
            " |      ```\n",
            " |  \n",
            " |  skip(self, n: int) -> 'Dataset'\n",
            " |      Create a new [`Dataset`] that skips the first `n` elements.\n",
            " |      \n",
            " |      Args:\n",
            " |          n (`int`):\n",
            " |              Number of elements to skip.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
            " |      >>> list(ds.take(3))\n",
            " |      [{'label': 1,\n",
            " |       'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
            " |       {'label': 1,\n",
            " |       'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
            " |       {'label': 1, 'text': 'effective but too-tepid biopic'}]\n",
            " |      >>> ds = ds.skip(1)\n",
            " |      >>> list(ds.take(3))\n",
            " |      [{'label': 1,\n",
            " |       'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'},\n",
            " |       {'label': 1, 'text': 'effective but too-tepid biopic'},\n",
            " |       {'label': 1,\n",
            " |       'text': 'if you sometimes like to go to the movies to have fun , wasabi is a good place to start .'}]\n",
            " |      ```\n",
            " |  \n",
            " |  sort(self, column_names: Union[str, Sequence[str]], reverse: Union[bool, Sequence[bool]] = False, kind='deprecated', null_placement: str = 'at_end', keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, new_fingerprint: Optional[str] = None) -> 'Dataset'\n",
            " |      Create a new dataset sorted according to a single or multiple columns.\n",
            " |      \n",
            " |      Args:\n",
            " |          column_names (`Union[str, Sequence[str]]`):\n",
            " |              Column name(s) to sort by.\n",
            " |          reverse (`Union[bool, Sequence[bool]]`, defaults to `False`):\n",
            " |              If `True`, sort by descending order rather than ascending. If a single bool is provided,\n",
            " |              the value is applied to the sorting of all column names. Otherwise a list of bools with the\n",
            " |              same length and order as column_names must be provided.\n",
            " |          kind (`str`, *optional*):\n",
            " |              Pandas algorithm for sorting selected in `{quicksort, mergesort, heapsort, stable}`,\n",
            " |              The default is `quicksort`. Note that both `stable` and `mergesort` use `timsort` under the covers and, in general,\n",
            " |              the actual implementation will vary with data type. The `mergesort` option is retained for backwards compatibility.\n",
            " |              <Deprecated version=\"2.8.0\">\n",
            " |      \n",
            " |              `kind` was deprecated in version 2.10.0 and will be removed in 3.0.0.\n",
            " |      \n",
            " |              </Deprecated>\n",
            " |          null_placement (`str`, defaults to `at_end`):\n",
            " |              Put `None` values at the beginning if `at_start` or `first` or at the end if `at_end` or `last`\n",
            " |      \n",
            " |              <Added version=\"1.14.2\"/>\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the sorted indices in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the sorted indices\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          indices_cache_file_name (`str`, *optional*, defaults to `None`):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              sorted indices instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              Higher value gives smaller cache files, lower value consume less temporary memory.\n",
            " |          new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the dataset after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset('rotten_tomatoes', split='validation')\n",
            " |      >>> ds['label'][:10]\n",
            " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " |      >>> sorted_ds = ds.sort('label')\n",
            " |      >>> sorted_ds['label'][:10]\n",
            " |      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            " |      >>> another_sorted_ds = ds.sort(['label', 'text'], reverse=[True, False])\n",
            " |      >>> another_sorted_ds['label'][:10]\n",
            " |      [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            " |      ```\n",
            " |  \n",
            " |  take(self, n: int) -> 'Dataset'\n",
            " |      Create a new [`Dataset`] with only the first `n` elements.\n",
            " |      \n",
            " |      Args:\n",
            " |          n (`int`):\n",
            " |              Number of elements to take.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"train\")\n",
            " |      >>> small_ds = ds.take(2)\n",
            " |      >>> list(small_ds)\n",
            " |      [{'label': 1,\n",
            " |       'text': 'the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'},\n",
            " |       {'label': 1,\n",
            " |       'text': 'the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .'}]\n",
            " |      ```\n",
            " |  \n",
            " |  to_csv(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, storage_options: Optional[dict] = None, **to_csv_kwargs) -> int\n",
            " |      Exports the dataset to csv\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
            " |              Either a path to a file (e.g. `file.csv`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.csv`),\n",
            " |              or a BinaryIO, where the dataset will be saved to in the specified format.\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of the batch to load in memory and write at once.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |          num_proc (`int`, *optional*):\n",
            " |              Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing. `batch_size` in this case defaults to\n",
            " |              `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
            " |              value if you have sufficient compute power.\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.19.0\"/>\n",
            " |          **to_csv_kwargs (additional keyword arguments):\n",
            " |              Parameters to pass to pandas's [`pandas.DataFrame.to_csv`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n",
            " |      \n",
            " |              <Changed version=\"2.10.0\">\n",
            " |      \n",
            " |              Now, `index` defaults to `False` if not specified.\n",
            " |      \n",
            " |              If you would like to write the index, pass `index=True` and also set a name for the index column by\n",
            " |              passing `index_label`.\n",
            " |      \n",
            " |              </Changed>\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of characters or bytes written.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_csv(\"path/to/dataset/directory\")\n",
            " |      ```\n",
            " |  \n",
            " |  to_dict(self, batch_size: Optional[int] = None, batched='deprecated') -> Union[dict, Iterator[dict]]\n",
            " |      Returns the dataset as a Python dict. Can also return a generator for large datasets.\n",
            " |      \n",
            " |      Args:\n",
            " |          batched (`bool`):\n",
            " |              Set to `True` to return a generator that yields the dataset as batches\n",
            " |              of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n",
            " |      \n",
            " |              <Deprecated version=\"2.11.0\">\n",
            " |      \n",
            " |              Use `.iter(batch_size=batch_size)` followed by `.to_dict()` on the individual batches instead.\n",
            " |      \n",
            " |              </Deprecated>\n",
            " |      \n",
            " |          batch_size (`int`, *optional*): The size (number of rows) of the batches if `batched` is `True`.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `dict` or `Iterator[dict]`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_dict()\n",
            " |      ```\n",
            " |  \n",
            " |  to_iterable_dataset(self, num_shards: Optional[int] = 1) -> 'IterableDataset'\n",
            " |      Get an [`datasets.IterableDataset`] from a map-style [`datasets.Dataset`].\n",
            " |      This is equivalent to loading a dataset in streaming mode with [`datasets.load_dataset`], but much faster since the data is streamed from local files.\n",
            " |      \n",
            " |      Contrary to map-style datasets, iterable datasets are lazy and can only be iterated over (e.g. using a for loop).\n",
            " |      Since they are read sequentially in training loops, iterable datasets are much faster than map-style datasets.\n",
            " |      All the transformations applied to iterable datasets like filtering or processing are done on-the-fly when you start iterating over the dataset.\n",
            " |      \n",
            " |      Still, it is possible to shuffle an iterable dataset using [`datasets.IterableDataset.shuffle`].\n",
            " |      This is a fast approximate shuffling that works best if you have multiple shards and if you specify a buffer size that is big enough.\n",
            " |      \n",
            " |      To get the best speed performance, make sure your dataset doesn't have an indices mapping.\n",
            " |      If this is the case, the data are not read contiguously, which can be slow sometimes.\n",
            " |      You can use `ds = ds.flatten_indices()` to write your dataset in contiguous chunks of data and have optimal speed before switching to an iterable dataset.\n",
            " |      \n",
            " |      Args:\n",
            " |          num_shards (`int`, default to `1`):\n",
            " |              Number of shards to define when instantiating the iterable dataset. This is especially useful for big datasets to be able to shuffle properly,\n",
            " |              and also to enable fast parallel loading using a PyTorch DataLoader or in distributed setups for example.\n",
            " |              Shards are defined using [`datasets.Dataset.shard`]: it simply slices the data without writing anything on disk.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`datasets.IterableDataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      Basic usage:\n",
            " |      ```python\n",
            " |      >>> ids = ds.to_iterable_dataset()\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      With lazy filtering and processing:\n",
            " |      ```python\n",
            " |      >>> ids = ds.to_iterable_dataset()\n",
            " |      >>> ids = ids.filter(filter_fn).map(process_fn)  # will filter and process on-the-fly when you start iterating over the iterable dataset\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      With sharding to enable efficient shuffling:\n",
            " |      ```python\n",
            " |      >>> ids = ds.to_iterable_dataset(num_shards=64)  # the dataset is split into 64 shards to be iterated over\n",
            " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer for fast approximate shuffling when you start iterating\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      With a PyTorch DataLoader:\n",
            " |      ```python\n",
            " |      >>> import torch\n",
            " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
            " |      >>> ids = ids.filter(filter_fn).map(process_fn)\n",
            " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards to each worker to load, filter and process when you start iterating\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      With a PyTorch DataLoader and shuffling:\n",
            " |      ```python\n",
            " |      >>> import torch\n",
            " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
            " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
            " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from the shuffled list of shards to each worker when you start iterating\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      In a distributed setup like PyTorch DDP with a PyTorch DataLoader and shuffling\n",
            " |      ```python\n",
            " |      >>> from datasets.distributed import split_dataset_by_node\n",
            " |      >>> ids = ds.to_iterable_dataset(num_shards=512)\n",
            " |      >>> ids = ids.shuffle(buffer_size=10_000)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
            " |      >>> ids = split_dataset_by_node(ds, world_size=8, rank=0)  # will keep only 512 / 8 = 64 shards from the shuffled lists of shards when you start iterating\n",
            " |      >>> dataloader = torch.utils.data.DataLoader(ids, num_workers=4)  # will assign 64 / 4 = 16 shards from this node's list of shards to each worker when you start iterating\n",
            " |      >>> for example in ids:\n",
            " |      ...     pass\n",
            " |      ```\n",
            " |      \n",
            " |      With shuffling and multiple epochs:\n",
            " |      ```python\n",
            " |      >>> ids = ds.to_iterable_dataset(num_shards=64)\n",
            " |      >>> ids = ids.shuffle(buffer_size=10_000, seed=42)  # will shuffle the shards order and use a shuffle buffer when you start iterating\n",
            " |      >>> for epoch in range(n_epochs):\n",
            " |      ...     ids.set_epoch(epoch)  # will use effective_seed = seed + epoch to shuffle the shards and for the shuffle buffer when you start iterating\n",
            " |      ...     for example in ids:\n",
            " |      ...         pass\n",
            " |      ```\n",
            " |      Feel free to also use [`IterableDataset.set_epoch`] when using a PyTorch DataLoader or in distributed setups.\n",
            " |  \n",
            " |  to_json(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, num_proc: Optional[int] = None, storage_options: Optional[dict] = None, **to_json_kwargs) -> int\n",
            " |      Export the dataset to JSON Lines or JSON.\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
            " |              Either a path to a file (e.g. `file.json`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.json`),\n",
            " |              or a BinaryIO, where the dataset will be saved to in the specified format.\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of the batch to load in memory and write at once.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |          num_proc (`int`, *optional*):\n",
            " |              Number of processes for multiprocessing. By default it doesn't\n",
            " |              use multiprocessing. `batch_size` in this case defaults to\n",
            " |              `datasets.config.DEFAULT_MAX_BATCH_SIZE` but feel free to make it 5x or 10x of the default\n",
            " |              value if you have sufficient compute power.\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.19.0\"/>\n",
            " |          **to_json_kwargs (additional keyword arguments):\n",
            " |              Parameters to pass to pandas's [`pandas.DataFrame.to_json`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_json.html).\n",
            " |      \n",
            " |              <Changed version=\"2.11.0\">\n",
            " |      \n",
            " |              Now, `index` defaults to `False` if `orient` is `\"split\"` or `\"table\"`.\n",
            " |      \n",
            " |              If you would like to write the index, pass `index=True`.\n",
            " |      \n",
            " |              </Changed>\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of characters or bytes written.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_json(\"path/to/dataset/directory\")\n",
            " |      ```\n",
            " |  \n",
            " |  to_list(self) -> list\n",
            " |      Returns the dataset as a Python list.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `list`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_list()\n",
            " |      ```\n",
            " |  \n",
            " |  to_pandas(self, batch_size: Optional[int] = None, batched: bool = False) -> Union[pandas.core.frame.DataFrame, Iterator[pandas.core.frame.DataFrame]]\n",
            " |      Returns the dataset as a `pandas.DataFrame`. Can also return a generator for large datasets.\n",
            " |      \n",
            " |      Args:\n",
            " |          batched (`bool`):\n",
            " |              Set to `True` to return a generator that yields the dataset as batches\n",
            " |              of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              The size (number of rows) of the batches if `batched` is `True`.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `pandas.DataFrame` or `Iterator[pandas.DataFrame]`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_pandas()\n",
            " |      ```\n",
            " |  \n",
            " |  to_parquet(self, path_or_buf: Union[str, bytes, os.PathLike, BinaryIO], batch_size: Optional[int] = None, storage_options: Optional[dict] = None, **parquet_writer_kwargs) -> int\n",
            " |      Exports the dataset to parquet\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_buf (`PathLike` or `FileOrBuffer`):\n",
            " |              Either a path to a file (e.g. `file.parquet`), a remote URI (e.g. `hf://datasets/username/my_dataset_name/data.parquet`),\n",
            " |              or a BinaryIO, where the dataset will be saved to in the specified format.\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of the batch to load in memory and write at once.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.19.0\"/>\n",
            " |          **parquet_writer_kwargs (additional keyword arguments):\n",
            " |              Parameters to pass to PyArrow's `pyarrow.parquet.ParquetWriter`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of characters or bytes written.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_parquet(\"path/to/dataset/directory\")\n",
            " |      ```\n",
            " |  \n",
            " |  to_polars(self, batch_size: Optional[int] = None, batched: bool = False, schema_overrides: Optional[dict] = None, rechunk: bool = True) -> Union[ForwardRef('pl.DataFrame'), Iterator[ForwardRef('pl.DataFrame')]]\n",
            " |      Returns the dataset as a `polars.DataFrame`. Can also return a generator for large datasets.\n",
            " |      \n",
            " |      Args:\n",
            " |          batched (`bool`):\n",
            " |              Set to `True` to return a generator that yields the dataset as batches\n",
            " |              of `batch_size` rows. Defaults to `False` (returns the whole datasets once).\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              The size (number of rows) of the batches if `batched` is `True`.\n",
            " |              Defaults to `genomicsml.datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |          schema_overrides (`dict`, *optional*):\n",
            " |              Support type specification or override of one or more columns; note that\n",
            " |              any dtypes inferred from the schema param will be overridden.\n",
            " |          rechunk (`bool`):\n",
            " |              Make sure that all data is in contiguous memory. Defaults to `True`.\n",
            " |      Returns:\n",
            " |          `polars.DataFrame` or `Iterator[polars.DataFrame]`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds.to_polars()\n",
            " |      ```\n",
            " |  \n",
            " |  to_sql(self, name: str, con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], batch_size: Optional[int] = None, **sql_writer_kwargs) -> int\n",
            " |      Exports the dataset to a SQL database.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (`str`):\n",
            " |              Name of SQL table.\n",
            " |          con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n",
            " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) or a SQLite3/SQLAlchemy connection object used to write to a database.\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of the batch to load in memory and write at once.\n",
            " |              Defaults to `datasets.config.DEFAULT_MAX_BATCH_SIZE`.\n",
            " |          **sql_writer_kwargs (additional keyword arguments):\n",
            " |              Parameters to pass to pandas's [`pandas.DataFrame.to_sql`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_sql.html).\n",
            " |      \n",
            " |              <Changed version=\"2.11.0\">\n",
            " |      \n",
            " |              Now, `index` defaults to `False` if not specified.\n",
            " |      \n",
            " |              If you would like to write the index, pass `index=True` and also set a name for the index column by\n",
            " |              passing `index_label`.\n",
            " |      \n",
            " |              </Changed>\n",
            " |      \n",
            " |      Returns:\n",
            " |          `int`: The number of records written.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> # con provided as a connection URI string\n",
            " |      >>> ds.to_sql(\"data\", \"sqlite:///my_own_db.sql\")\n",
            " |      >>> # con provided as a sqlite3 connection object\n",
            " |      >>> import sqlite3\n",
            " |      >>> con = sqlite3.connect(\"my_own_db.sql\")\n",
            " |      >>> with con:\n",
            " |      ...     ds.to_sql(\"data\", con)\n",
            " |      ```\n",
            " |  \n",
            " |  train_test_split(self, test_size: Union[float, int, NoneType] = None, train_size: Union[float, int, NoneType] = None, shuffle: bool = True, stratify_by_column: Optional[str] = None, seed: Optional[int] = None, generator: Optional[numpy.random._generator.Generator] = None, keep_in_memory: bool = False, load_from_cache_file: Optional[bool] = None, train_indices_cache_file_name: Optional[str] = None, test_indices_cache_file_name: Optional[str] = None, writer_batch_size: Optional[int] = 1000, train_new_fingerprint: Optional[str] = None, test_new_fingerprint: Optional[str] = None) -> 'DatasetDict'\n",
            " |      Return a dictionary ([`datasets.DatasetDict`]) with two random train and test subsets (`train` and `test` `Dataset` splits).\n",
            " |      Splits are created from the dataset according to `test_size`, `train_size` and `shuffle`.\n",
            " |      \n",
            " |      This method is similar to scikit-learn `train_test_split`.\n",
            " |      \n",
            " |      Args:\n",
            " |          test_size (`numpy.random.Generator`, *optional*):\n",
            " |              Size of the test split\n",
            " |              If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the test split.\n",
            " |              If `int`, represents the absolute number of test samples.\n",
            " |              If `None`, the value is set to the complement of the train size.\n",
            " |              If `train_size` is also `None`, it will be set to `0.25`.\n",
            " |          train_size (`numpy.random.Generator`, *optional*):\n",
            " |              Size of the train split\n",
            " |              If `float`, should be between `0.0` and `1.0` and represent the proportion of the dataset to include in the train split.\n",
            " |              If `int`, represents the absolute number of train samples.\n",
            " |              If `None`, the value is automatically set to the complement of the test size.\n",
            " |          shuffle (`bool`, *optional*, defaults to `True`):\n",
            " |              Whether or not to shuffle the data before splitting.\n",
            " |          stratify_by_column (`str`, *optional*, defaults to `None`):\n",
            " |              The column name of labels to be used to perform stratified split of data.\n",
            " |          seed (`int`, *optional*):\n",
            " |              A seed to initialize the default BitGenerator if `generator=None`.\n",
            " |              If `None`, then fresh, unpredictable entropy will be pulled from the OS.\n",
            " |              If an `int` or `array_like[ints]` is passed, then it will be passed to SeedSequence to derive the initial BitGenerator state.\n",
            " |          generator (`numpy.random.Generator`, *optional*):\n",
            " |              Numpy random Generator to use to compute the permutation of the dataset rows.\n",
            " |              If `generator=None` (default), uses `np.random.default_rng` (the default BitGenerator (PCG64) of NumPy).\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Keep the splits indices in memory instead of writing it to a cache file.\n",
            " |          load_from_cache_file (`Optional[bool]`, defaults to `True` if caching is enabled):\n",
            " |              If a cache file storing the splits indices\n",
            " |              can be identified, use it instead of recomputing.\n",
            " |          train_cache_file_name (`str`, *optional*):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              train split indices instead of the automatically generated cache file name.\n",
            " |          test_cache_file_name (`str`, *optional*):\n",
            " |              Provide the name of a path for the cache file. It is used to store the\n",
            " |              test split indices instead of the automatically generated cache file name.\n",
            " |          writer_batch_size (`int`, defaults to `1000`):\n",
            " |              Number of rows per write operation for the cache file writer.\n",
            " |              This value is a good trade-off between memory usage during the processing, and processing speed.\n",
            " |              Higher value makes the processing do fewer lookups, lower value consume less temporary memory while running `map`.\n",
            " |          train_new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the train set after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
            " |          test_new_fingerprint (`str`, *optional*, defaults to `None`):\n",
            " |              The new fingerprint of the test set after transform.\n",
            " |              If `None`, the new fingerprint is computed using a hash of the previous fingerprint, and the transform arguments\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds = ds.train_test_split(test_size=0.2, shuffle=True)\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 852\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 214\n",
            " |          })\n",
            " |      })\n",
            " |      \n",
            " |      # set a seed\n",
            " |      >>> ds = ds.train_test_split(test_size=0.2, seed=42)\n",
            " |      \n",
            " |      # stratified split\n",
            " |      >>> ds = load_dataset(\"imdb\",split=\"train\")\n",
            " |      Dataset({\n",
            " |          features: ['text', 'label'],\n",
            " |          num_rows: 25000\n",
            " |      })\n",
            " |      >>> ds = ds.train_test_split(test_size=0.2, stratify_by_column=\"label\")\n",
            " |      DatasetDict({\n",
            " |          train: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 20000\n",
            " |          })\n",
            " |          test: Dataset({\n",
            " |              features: ['text', 'label'],\n",
            " |              num_rows: 5000\n",
            " |          })\n",
            " |      })\n",
            " |      ```\n",
            " |  \n",
            " |  unique(self, column: str) -> List\n",
            " |      Return a list of the unique elements in a column.\n",
            " |      \n",
            " |      This is implemented in the low-level backend and as such, very fast.\n",
            " |      \n",
            " |      Args:\n",
            " |          column (`str`):\n",
            " |              Column name (list all the column names with [`~datasets.Dataset.column_names`]).\n",
            " |      \n",
            " |      Returns:\n",
            " |          `list`: List of unique elements in the given column.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.unique('label')\n",
            " |      [1, 0]\n",
            " |      ```\n",
            " |  \n",
            " |  with_format(self, type: Optional[str] = None, columns: Optional[List] = None, output_all_columns: bool = False, **format_kwargs)\n",
            " |      Set `__getitem__` return format (type and columns). The data formatting is applied on-the-fly.\n",
            " |      The format `type` (for example \"numpy\") is used to format batches when using `__getitem__`.\n",
            " |      \n",
            " |      It's also possible to use custom transforms for formatting using [`~datasets.Dataset.with_transform`].\n",
            " |      \n",
            " |      Contrary to [`~datasets.Dataset.set_format`], `with_format` returns a new [`Dataset`] object.\n",
            " |      \n",
            " |      Args:\n",
            " |          type (`str`, *optional*):\n",
            " |              Either output type selected in `[None, 'numpy', 'torch', 'tensorflow', 'pandas', 'arrow', 'jax']`.\n",
            " |              `None` means `__getitem__` returns python objects (default).\n",
            " |          columns (`List[str]`, *optional*):\n",
            " |              Columns to format in the output.\n",
            " |              `None` means `__getitem__` returns all columns (default).\n",
            " |          output_all_columns (`bool`, defaults to `False`):\n",
            " |              Keep un-formatted columns as well in the output (as python objects).\n",
            " |          **format_kwargs (additional keyword arguments):\n",
            " |              Keywords arguments passed to the convert function like `np.array`, `torch.tensor` or `tensorflow.ragged.constant`.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> ds = ds.map(lambda x: tokenizer(x['text'], truncation=True, padding=True), batched=True)\n",
            " |      >>> ds.format\n",
            " |      {'columns': ['text', 'label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': None}\n",
            " |      >>> ds = ds.with_format(type='tensorflow', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'])\n",
            " |      >>> ds.format\n",
            " |      {'columns': ['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |       'format_kwargs': {},\n",
            " |       'output_all_columns': False,\n",
            " |       'type': 'tensorflow'}\n",
            " |      ```\n",
            " |  \n",
            " |  with_transform(self, transform: Optional[Callable], columns: Optional[List] = None, output_all_columns: bool = False)\n",
            " |      Set `__getitem__` return format using this transform. The transform is applied on-the-fly on batches when `__getitem__` is called.\n",
            " |      \n",
            " |      As [`~datasets.Dataset.set_format`], this can be reset using [`~datasets.Dataset.reset_format`].\n",
            " |      \n",
            " |      Contrary to [`~datasets.Dataset.set_transform`], `with_transform` returns a new [`Dataset`] object.\n",
            " |      \n",
            " |      Args:\n",
            " |          transform (`Callable`, `optional`):\n",
            " |              User-defined formatting transform, replaces the format defined by [`~datasets.Dataset.set_format`].\n",
            " |              A formatting function is a callable that takes a batch (as a `dict`) as input and returns a batch.\n",
            " |              This function is applied right before returning the objects in `__getitem__`.\n",
            " |          columns (`List[str]`, `optional`):\n",
            " |              Columns to format in the output.\n",
            " |              If specified, then the input batch of the transform only contains those columns.\n",
            " |          output_all_columns (`bool`, defaults to `False`):\n",
            " |              Keep un-formatted columns as well in the output (as python objects).\n",
            " |              If set to `True`, then the other un-formatted columns are kept with the output of the transform.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> from transformers import AutoTokenizer\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
            " |      >>> def encode(example):\n",
            " |      ...     return tokenizer(example[\"text\"], padding=True, truncation=True, return_tensors='pt')\n",
            " |      >>> ds = ds.with_transform(encode)\n",
            " |      >>> ds[0]\n",
            " |      {'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            " |       1, 1, 1, 1, 1]),\n",
            " |       'input_ids': tensor([  101, 18027, 16310, 16001,  1103,  9321,   178, 11604,  7235,  6617,\n",
            " |               1742,  2165,  2820,  1206,  6588, 22572, 12937,  1811,  2153,  1105,\n",
            " |               1147, 12890, 19587,  6463,  1105, 15026,  1482,   119,   102]),\n",
            " |       'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            " |               0, 0, 0, 0, 0])}\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods defined here:\n",
            " |  \n",
            " |  from_buffer(buffer: pyarrow.lib.Buffer, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_buffer: Optional[pyarrow.lib.Buffer] = None) -> 'Dataset' from builtins.type\n",
            " |      Instantiate a Dataset backed by an Arrow buffer.\n",
            " |      \n",
            " |      Args:\n",
            " |          buffer (`pyarrow.Buffer`):\n",
            " |              Arrow buffer.\n",
            " |          info (`DatasetInfo`, *optional*):\n",
            " |              Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Name of the dataset split.\n",
            " |          indices_buffer (`pyarrow.Buffer`, *optional*):\n",
            " |              Indices Arrow buffer.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |  \n",
            " |  from_dict(mapping: dict, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
            " |      Convert `dict` to a `pyarrow.Table` to create a [`Dataset`].\n",
            " |      \n",
            " |      Args:\n",
            " |          mapping (`Mapping`):\n",
            " |              Mapping of strings to Arrays or Python lists.\n",
            " |          features ([`Features`], *optional*):\n",
            " |              Dataset features.\n",
            " |          info (`DatasetInfo`, *optional*):\n",
            " |              Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Name of the dataset split.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |  \n",
            " |  from_file(filename: str, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, indices_filename: Optional[str] = None, in_memory: bool = False) -> 'Dataset' from builtins.type\n",
            " |      Instantiate a Dataset backed by an Arrow table at filename.\n",
            " |      \n",
            " |      Args:\n",
            " |          filename (`str`):\n",
            " |              File name of the dataset.\n",
            " |          info (`DatasetInfo`, *optional*):\n",
            " |              Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Name of the dataset split.\n",
            " |          indices_filename (`str`, *optional*):\n",
            " |              File names of the indices.\n",
            " |          in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |  \n",
            " |  from_list(mapping: List[dict], features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
            " |      Convert a list of dicts to a `pyarrow.Table` to create a [`Dataset`]`.\n",
            " |      \n",
            " |      Note that the keys of the first entry will be used to determine the dataset columns,\n",
            " |      regardless of what is passed to features.\n",
            " |      \n",
            " |      Args:\n",
            " |          mapping (`List[dict]`): A list of mappings of strings to row values.\n",
            " |          features (`Features`, optional): Dataset features.\n",
            " |          info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, optional): Name of the dataset split.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |  \n",
            " |  from_pandas(df: pandas.core.frame.DataFrame, features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None, preserve_index: Optional[bool] = None) -> 'Dataset' from builtins.type\n",
            " |      Convert `pandas.DataFrame` to a `pyarrow.Table` to create a [`Dataset`].\n",
            " |      \n",
            " |      The column types in the resulting Arrow Table are inferred from the dtypes of the `pandas.Series` in the\n",
            " |      DataFrame. In the case of non-object Series, the NumPy dtype is translated to its Arrow equivalent. In the\n",
            " |      case of `object`, we need to guess the datatype by looking at the Python objects in this Series.\n",
            " |      \n",
            " |      Be aware that Series of the `object` dtype don't carry enough information to always lead to a meaningful Arrow\n",
            " |      type. In the case that we cannot infer a type, e.g. because the DataFrame is of length 0 or the Series only\n",
            " |      contains `None/nan` objects, the type is set to `null`. This behavior can be avoided by constructing explicit\n",
            " |      features and passing it to this function.\n",
            " |      \n",
            " |      Args:\n",
            " |          df (`pandas.DataFrame`):\n",
            " |              Dataframe that contains the dataset.\n",
            " |          features ([`Features`], *optional*):\n",
            " |              Dataset features.\n",
            " |          info (`DatasetInfo`, *optional*):\n",
            " |              Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Name of the dataset split.\n",
            " |          preserve_index (`bool`, *optional*):\n",
            " |              Whether to store the index as an additional column in the resulting Dataset.\n",
            " |              The default of `None` will store the index as a column, except for `RangeIndex` which is stored as metadata only.\n",
            " |              Use `preserve_index=True` to force it to be stored as a column.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_pandas(df)\n",
            " |      ```\n",
            " |  \n",
            " |  from_polars(df: 'pl.DataFrame', features: Optional[datasets.features.features.Features] = None, info: Optional[datasets.info.DatasetInfo] = None, split: Optional[datasets.splits.NamedSplit] = None) -> 'Dataset' from builtins.type\n",
            " |      Collect the underlying arrow arrays in an Arrow Table.\n",
            " |      \n",
            " |      This operation is mostly zero copy.\n",
            " |      \n",
            " |      Data types that do copy:\n",
            " |          * CategoricalType\n",
            " |      \n",
            " |      Args:\n",
            " |          df (`polars.DataFrame`): DataFrame to convert to Arrow Table\n",
            " |          features (`Features`, optional): Dataset features.\n",
            " |          info (`DatasetInfo`, optional): Dataset information, like description, citation, etc.\n",
            " |          split (`NamedSplit`, optional): Name of the dataset split.\n",
            " |      \n",
            " |      Examples:\n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_polars(df)\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods defined here:\n",
            " |  \n",
            " |  from_csv(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, num_proc: Optional[int] = None, **kwargs)\n",
            " |      Create Dataset from CSV file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (`path-like` or list of `path-like`):\n",
            " |              Path(s) of the CSV file(s).\n",
            " |          split ([`NamedSplit`], *optional*):\n",
            " |              Split name to be assigned to the dataset.\n",
            " |          features ([`Features`], *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to [`pandas.read_csv`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_csv('path/to/dataset.csv')\n",
            " |      ```\n",
            " |  \n",
            " |  from_generator(generator: Callable, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, gen_kwargs: Optional[dict] = None, num_proc: Optional[int] = None, **kwargs)\n",
            " |      Create a Dataset from a generator.\n",
            " |      \n",
            " |      Args:\n",
            " |          generator (:`Callable`):\n",
            " |              A generator function that `yields` examples.\n",
            " |          features ([`Features`], *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          gen_kwargs(`dict`, *optional*):\n",
            " |              Keyword arguments to be passed to the `generator` callable.\n",
            " |              You can define a sharded dataset by passing the list of shards in `gen_kwargs` and setting `num_proc` greater than 1.\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
            " |              If `num_proc` is greater than one, then all list values in `gen_kwargs` must be the same length. These values will be split between calls to the generator. The number of shards will be the minimum of the shortest list in `gen_kwargs` and `num_proc`.\n",
            " |      \n",
            " |              <Added version=\"2.7.0\"/>\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to :[`GeneratorConfig`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> def gen():\n",
            " |      ...     yield {\"text\": \"Good\", \"label\": 0}\n",
            " |      ...     yield {\"text\": \"Bad\", \"label\": 1}\n",
            " |      ...\n",
            " |      >>> ds = Dataset.from_generator(gen)\n",
            " |      ```\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> def gen(shards):\n",
            " |      ...     for shard in shards:\n",
            " |      ...         with open(shard) as f:\n",
            " |      ...             for line in f:\n",
            " |      ...                 yield {\"line\": line}\n",
            " |      ...\n",
            " |      >>> shards = [f\"data{i}.txt\" for i in range(32)]\n",
            " |      >>> ds = Dataset.from_generator(gen, gen_kwargs={\"shards\": shards})\n",
            " |      ```\n",
            " |  \n",
            " |  from_json(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, field: Optional[str] = None, num_proc: Optional[int] = None, **kwargs)\n",
            " |      Create Dataset from JSON or JSON Lines file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (`path-like` or list of `path-like`):\n",
            " |              Path(s) of the JSON or JSON Lines file(s).\n",
            " |          split ([`NamedSplit`], *optional*):\n",
            " |              Split name to be assigned to the dataset.\n",
            " |          features ([`Features`], *optional*):\n",
            " |               Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          field (`str`, *optional*):\n",
            " |              Field name of the JSON file where the dataset is contained in.\n",
            " |          num_proc (`int`, *optional* defaults to `None`):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to [`JsonConfig`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_json('path/to/dataset.json')\n",
            " |      ```\n",
            " |  \n",
            " |  from_parquet(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, columns: Optional[List[str]] = None, num_proc: Optional[int] = None, **kwargs)\n",
            " |      Create Dataset from Parquet file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (`path-like` or list of `path-like`):\n",
            " |              Path(s) of the Parquet file(s).\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Split name to be assigned to the dataset.\n",
            " |          features (`Features`, *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          columns (`List[str]`, *optional*):\n",
            " |              If not `None`, only these columns will be read from the file.\n",
            " |              A column name may be a prefix of a nested field, e.g. 'a' will select\n",
            " |              'a.b', 'a.c', and 'a.d.e'.\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to [`ParquetConfig`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_parquet('path/to/dataset.parquet')\n",
            " |      ```\n",
            " |  \n",
            " |  from_spark(df: 'pyspark.sql.DataFrame', split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, keep_in_memory: bool = False, cache_dir: str = None, working_dir: str = None, load_from_cache_file: bool = True, **kwargs)\n",
            " |      Create a Dataset from Spark DataFrame. Dataset downloading is distributed over Spark workers.\n",
            " |      \n",
            " |      Args:\n",
            " |          df (`pyspark.sql.DataFrame`):\n",
            " |              The DataFrame containing the desired data.\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Split name to be assigned to the dataset.\n",
            " |          features (`Features`, *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data. When using a multi-node Spark cluster, the cache_dir must be accessible to both\n",
            " |              workers and the driver.\n",
            " |          keep_in_memory (`bool`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          working_dir (`str`, *optional*)\n",
            " |              Intermediate directory for each Spark worker to write data to before moving it to `cache_dir`. Setting\n",
            " |              a non-NFS intermediate directory may improve performance.\n",
            " |          load_from_cache_file (`bool`):\n",
            " |              Whether to load the dataset from the cache if possible.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> df = spark.createDataFrame(\n",
            " |      >>>     data=[[1, \"Elia\"], [2, \"Teo\"], [3, \"Fang\"]],\n",
            " |      >>>     columns=[\"id\", \"name\"],\n",
            " |      >>> )\n",
            " |      >>> ds = Dataset.from_spark(df)\n",
            " |      ```\n",
            " |  \n",
            " |  from_sql(sql: Union[str, ForwardRef('sqlalchemy.sql.Selectable')], con: Union[str, ForwardRef('sqlalchemy.engine.Connection'), ForwardRef('sqlalchemy.engine.Engine'), ForwardRef('sqlite3.Connection')], features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, **kwargs)\n",
            " |      Create Dataset from SQL query or database table.\n",
            " |      \n",
            " |      Args:\n",
            " |          sql (`str` or `sqlalchemy.sql.Selectable`):\n",
            " |              SQL query to be executed or a table name.\n",
            " |          con (`str` or `sqlite3.Connection` or `sqlalchemy.engine.Connection` or `sqlalchemy.engine.Connection`):\n",
            " |              A [URI string](https://docs.sqlalchemy.org/en/13/core/engines.html#database-urls) used to instantiate a database connection or a SQLite3/SQLAlchemy connection object.\n",
            " |          features ([`Features`], *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to [`SqlConfig`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> # Fetch a database table\n",
            " |      >>> ds = Dataset.from_sql(\"test_data\", \"postgres:///db_name\")\n",
            " |      >>> # Execute a SQL query on the table\n",
            " |      >>> ds = Dataset.from_sql(\"SELECT sentence FROM test_data\", \"postgres:///db_name\")\n",
            " |      >>> # Use a Selectable object to specify the query\n",
            " |      >>> from sqlalchemy import select, text\n",
            " |      >>> stmt = select([text(\"sentence\")]).select_from(text(\"test_data\"))\n",
            " |      >>> ds = Dataset.from_sql(stmt, \"postgres:///db_name\")\n",
            " |      ```\n",
            " |      \n",
            " |      <Tip>\n",
            " |      \n",
            " |      The returned dataset can only be cached if `con` is specified as URI string.\n",
            " |      \n",
            " |      </Tip>\n",
            " |  \n",
            " |  from_text(path_or_paths: Union[str, bytes, os.PathLike, List[Union[str, bytes, os.PathLike]]], split: Optional[datasets.splits.NamedSplit] = None, features: Optional[datasets.features.features.Features] = None, cache_dir: str = None, keep_in_memory: bool = False, num_proc: Optional[int] = None, **kwargs)\n",
            " |      Create Dataset from text file(s).\n",
            " |      \n",
            " |      Args:\n",
            " |          path_or_paths (`path-like` or list of `path-like`):\n",
            " |              Path(s) of the text file(s).\n",
            " |          split (`NamedSplit`, *optional*):\n",
            " |              Split name to be assigned to the dataset.\n",
            " |          features (`Features`, *optional*):\n",
            " |              Dataset features.\n",
            " |          cache_dir (`str`, *optional*, defaults to `\"~/.cache/huggingface/datasets\"`):\n",
            " |              Directory to cache data.\n",
            " |          keep_in_memory (`bool`, defaults to `False`):\n",
            " |              Whether to copy the data in-memory.\n",
            " |          num_proc (`int`, *optional*, defaults to `None`):\n",
            " |              Number of processes when downloading and generating the dataset locally.\n",
            " |              This is helpful if the dataset is made of multiple files. Multiprocessing is disabled by default.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |          **kwargs (additional keyword arguments):\n",
            " |              Keyword arguments to be passed to [`TextConfig`].\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`]\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = Dataset.from_text('path/to/dataset.txt')\n",
            " |      ```\n",
            " |  \n",
            " |  load_from_disk(dataset_path: str, fs='deprecated', keep_in_memory: Optional[bool] = None, storage_options: Optional[dict] = None) -> 'Dataset'\n",
            " |      Loads a dataset that was previously saved using [`save_to_disk`] from a dataset directory, or from a\n",
            " |      filesystem using any implementation of `fsspec.spec.AbstractFileSystem`.\n",
            " |      \n",
            " |      Args:\n",
            " |          dataset_path (`str`):\n",
            " |              Path (e.g. `\"dataset/train\"`) or remote URI (e.g. `\"s3//my-bucket/dataset/train\"`)\n",
            " |              of the dataset directory where the dataset will be loaded from.\n",
            " |          fs (`fsspec.spec.AbstractFileSystem`, *optional*):\n",
            " |              Instance of the remote filesystem where the dataset will be saved to.\n",
            " |      \n",
            " |              <Deprecated version=\"2.8.0\">\n",
            " |      \n",
            " |              `fs` was deprecated in version 2.8.0 and will be removed in 3.0.0.\n",
            " |              Please use `storage_options` instead, e.g. `storage_options=fs.storage_options`\n",
            " |      \n",
            " |              </Deprecated>\n",
            " |      \n",
            " |          keep_in_memory (`bool`, defaults to `None`):\n",
            " |              Whether to copy the dataset in-memory. If `None`, the\n",
            " |              dataset will not be copied in-memory unless explicitly enabled by setting\n",
            " |              `datasets.config.IN_MEMORY_MAX_SIZE` to nonzero. See more details in the\n",
            " |              [improve performance](../cache#improve-performance) section.\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.8.0\"/>\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`Dataset`] or [`DatasetDict`]:\n",
            " |          - If `dataset_path` is a path of a dataset directory, the dataset requested.\n",
            " |          - If `dataset_path` is a path of a dataset dict directory, a `datasets.DatasetDict` with each split.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds = load_from_disk(\"path/to/dataset/directory\")\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties defined here:\n",
            " |  \n",
            " |  cache_files\n",
            " |      The cache files containing the Apache Arrow table backing the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.cache_files\n",
            " |      [{'filename': '/root/.cache/huggingface/datasets/rotten_tomatoes_movie_review/default/1.0.0/40d411e45a6ce3484deed7cc15b82a53dad9a72aafd9f86f8f227134bec5ca46/rotten_tomatoes_movie_review-validation.arrow'}]\n",
            " |      ```\n",
            " |  \n",
            " |  column_names\n",
            " |      Names of the columns in the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.column_names\n",
            " |      ['text', 'label']\n",
            " |      ```\n",
            " |  \n",
            " |  data\n",
            " |      The Apache Arrow table backing the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.data\n",
            " |      MemoryMappedTable\n",
            " |      text: string\n",
            " |      label: int64\n",
            " |      ----\n",
            " |      text: [[\"compassionately explores the seemingly irreconcilable situation between conservative christian parents and their estranged gay and lesbian children .\",\"the soundtrack alone is worth the price of admission .\",\"rodriguez does a splendid job of racial profiling hollywood style--casting excellent latin actors of all ages--a trend long overdue .\",\"beneath the film's obvious determination to shock at any cost lies considerable skill and determination , backed by sheer nerve .\",\"bielinsky is a filmmaker of impressive talent .\",\"so beautifully acted and directed , it's clear that washington most certainly has a new career ahead of him if he so chooses .\",\"a visual spectacle full of stunning images and effects .\",\"a gentle and engrossing character study .\",\"it's enough to watch huppert scheming , with her small , intelligent eyes as steady as any noir villain , and to enjoy the perfectly pitched web of tension that chabrol spins .\",\"an engrossing portrait of uncompromising artists trying to create something original against the backdrop of a corporate music industry that only seems to care about the bottom line .\",...,\"ultimately , jane learns her place as a girl , softens up and loses some of the intensity that made her an interesting character to begin with .\",\"ah-nuld's action hero days might be over .\",\"it's clear why deuces wild , which was shot two years ago , has been gathering dust on mgm's shelf .\",\"feels like nothing quite so much as a middle-aged moviemaker's attempt to surround himself with beautiful , half-naked women .\",\"when the precise nature of matthew's predicament finally comes into sharp focus , the revelation fails to justify the build-up .\",\"this picture is murder by numbers , and as easy to be bored by as your abc's , despite a few whopping shootouts .\",\"hilarious musical comedy though stymied by accents thick as mud .\",\"if you are into splatter movies , then you will probably have a reasonably good time with the salton sea .\",\"a dull , simple-minded and stereotypical tale of drugs , death and mind-numbing indifference on the inner-city streets .\",\"the feature-length stretch . . . strains the show's concept .\"]]\n",
            " |      label: [[1,1,1,1,1,1,1,1,1,1,...,0,0,0,0,0,0,0,0,0,0]]\n",
            " |      ```\n",
            " |  \n",
            " |  features\n",
            " |  \n",
            " |  format\n",
            " |  \n",
            " |  num_columns\n",
            " |      Number of columns in the dataset.\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.num_columns\n",
            " |      2\n",
            " |      ```\n",
            " |  \n",
            " |  num_rows\n",
            " |      Number of rows in the dataset (same as [`Dataset.__len__`]).\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.num_rows\n",
            " |      1066\n",
            " |      ```\n",
            " |  \n",
            " |  shape\n",
            " |      Shape of the dataset (number of columns, number of rows).\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> from datasets import load_dataset\n",
            " |      >>> ds = load_dataset(\"rotten_tomatoes\", split=\"validation\")\n",
            " |      >>> ds.shape\n",
            " |      (1066, 2)\n",
            " |      ```\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Readonly properties inherited from DatasetInfoMixin:\n",
            " |  \n",
            " |  builder_name\n",
            " |  \n",
            " |  citation\n",
            " |  \n",
            " |  config_name\n",
            " |  \n",
            " |  dataset_size\n",
            " |  \n",
            " |  description\n",
            " |  \n",
            " |  download_checksums\n",
            " |  \n",
            " |  download_size\n",
            " |  \n",
            " |  homepage\n",
            " |  \n",
            " |  info\n",
            " |      [`~datasets.DatasetInfo`] object containing all the metadata in the dataset.\n",
            " |  \n",
            " |  license\n",
            " |  \n",
            " |  size_in_bytes\n",
            " |  \n",
            " |  split\n",
            " |      [`~datasets.NamedSplit`] object corresponding to a named dataset split.\n",
            " |  \n",
            " |  supervised_keys\n",
            " |  \n",
            " |  task_templates\n",
            " |  \n",
            " |  version\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from DatasetInfoMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from datasets.search.IndexableMixin:\n",
            " |  \n",
            " |  drop_index(self, index_name: str)\n",
            " |      Drop the index with the specified column.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |  \n",
            " |  get_index(self, index_name: str) -> datasets.search.BaseIndex\n",
            " |      List the `index_name`/identifiers of all the attached indexes.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`): Index name.\n",
            " |      \n",
            " |      Returns:\n",
            " |          [`BaseIndex`]\n",
            " |  \n",
            " |  get_nearest_examples(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10, **kwargs) -> datasets.search.NearestExamplesResults\n",
            " |      Find the nearest examples in the dataset to the query.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The index_name/identifier of the index.\n",
            " |          query (`Union[str, np.ndarray]`):\n",
            " |              The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
            " |          k (`int`):\n",
            " |              The number of examples to retrieve.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `(scores, examples)`:\n",
            " |              A tuple of `(scores, examples)` where:\n",
            " |              - **scores** (`List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n",
            " |              - **examples** (`dict`): the retrieved examples\n",
            " |  \n",
            " |  get_nearest_examples_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10, **kwargs) -> datasets.search.BatchedNearestExamplesResults\n",
            " |      Find the nearest examples in the dataset to the query.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |          queries (`Union[List[str], np.ndarray]`):\n",
            " |              The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
            " |          k (`int`):\n",
            " |              The number of examples to retrieve per query.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `(total_scores, total_examples)`:\n",
            " |              A tuple of `(total_scores, total_examples)` where:\n",
            " |              - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n",
            " |              - **total_examples** (`List[dict]`): the retrieved examples per query\n",
            " |  \n",
            " |  is_index_initialized(self, index_name: str) -> bool\n",
            " |  \n",
            " |  list_indexes(self) -> List[str]\n",
            " |      List the `colindex_nameumns`/identifiers of all the attached indexes.\n",
            " |  \n",
            " |  load_elasticsearch_index(self, index_name: str, es_index_name: str, host: Optional[str] = None, port: Optional[int] = None, es_client: Optional[ForwardRef('Elasticsearch')] = None, es_index_config: Optional[dict] = None)\n",
            " |      Load an existing text index using ElasticSearch for fast retrieval.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The `index_name`/identifier of the index. This is the index name that is used to call `get_nearest` or `search`.\n",
            " |          es_index_name (`str`):\n",
            " |              The name of elasticsearch index to load.\n",
            " |          host (`str`, *optional*, defaults to `localhost`):\n",
            " |              Host of where ElasticSearch is running.\n",
            " |          port (`str`, *optional*, defaults to `9200`):\n",
            " |              Port of where ElasticSearch is running.\n",
            " |          es_client (`elasticsearch.Elasticsearch`, *optional*):\n",
            " |              The elasticsearch client used to create the index if host and port are `None`.\n",
            " |          es_index_config (`dict`, *optional*):\n",
            " |              The configuration of the elasticsearch index.\n",
            " |              Default config is:\n",
            " |                  ```\n",
            " |                  {\n",
            " |                      \"settings\": {\n",
            " |                          \"number_of_shards\": 1,\n",
            " |                          \"analysis\": {\"analyzer\": {\"stop_standard\": {\"type\": \"standard\", \" stopwords\": \"_english_\"}}},\n",
            " |                      },\n",
            " |                      \"mappings\": {\n",
            " |                          \"properties\": {\n",
            " |                              \"text\": {\n",
            " |                                  \"type\": \"text\",\n",
            " |                                  \"analyzer\": \"standard\",\n",
            " |                                  \"similarity\": \"BM25\"\n",
            " |                              },\n",
            " |                          }\n",
            " |                      },\n",
            " |                  }\n",
            " |                  ```\n",
            " |  \n",
            " |  load_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath], device: Union[int, List[int], NoneType] = None, storage_options: Optional[Dict] = None)\n",
            " |      Load a FaissIndex from disk.\n",
            " |      \n",
            " |      If you want to do additional configurations, you can have access to the faiss index object by doing\n",
            " |      `.get_index(index_name).faiss_index` to make it fit your needs.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to\n",
            " |              call `.get_nearest` or `.search`.\n",
            " |          file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `\"s3://my-bucket/index.faiss\"`).\n",
            " |          device (Optional `Union[int, List[int]]`): If positive integer, this is the index of the GPU to use. If negative integer, use all GPUs.\n",
            " |              If a list of positive integers is passed in, run only on those GPUs. By default it uses the CPU.\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.11.0\"/>\n",
            " |  \n",
            " |  save_faiss_index(self, index_name: str, file: Union[str, pathlib.PurePath], storage_options: Optional[Dict] = None)\n",
            " |      Save a FaissIndex on disk.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`): The index_name/identifier of the index. This is the index_name that is used to call `.get_nearest` or `.search`.\n",
            " |          file (`str`): The path to the serialized faiss index on disk or remote URI (e.g. `\"s3://my-bucket/index.faiss\"`).\n",
            " |          storage_options (`dict`, *optional*):\n",
            " |              Key/value pairs to be passed on to the file-system backend, if any.\n",
            " |      \n",
            " |              <Added version=\"2.11.0\"/>\n",
            " |  \n",
            " |  search(self, index_name: str, query: Union[str, <built-in function array>], k: int = 10, **kwargs) -> datasets.search.SearchResults\n",
            " |      Find the nearest examples indices in the dataset to the query.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The name/identifier of the index.\n",
            " |          query (`Union[str, np.ndarray]`):\n",
            " |              The query as a string if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
            " |          k (`int`):\n",
            " |              The number of examples to retrieve.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `(scores, indices)`:\n",
            " |              A tuple of `(scores, indices)` where:\n",
            " |              - **scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples\n",
            " |              - **indices** (`List[List[int]]`): the indices of the retrieved examples\n",
            " |  \n",
            " |  search_batch(self, index_name: str, queries: Union[List[str], <built-in function array>], k: int = 10, **kwargs) -> datasets.search.BatchedSearchResults\n",
            " |      Find the nearest examples indices in the dataset to the query.\n",
            " |      \n",
            " |      Args:\n",
            " |          index_name (`str`):\n",
            " |              The `index_name`/identifier of the index.\n",
            " |          queries (`Union[List[str], np.ndarray]`):\n",
            " |              The queries as a list of strings if `index_name` is a text index or as a numpy array if `index_name` is a vector index.\n",
            " |          k (`int`):\n",
            " |              The number of examples to retrieve per query.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `(total_scores, total_indices)`:\n",
            " |              A tuple of `(total_scores, total_indices)` where:\n",
            " |              - **total_scores** (`List[List[float]`): the retrieval scores from either FAISS (`IndexFlatL2` by default) or ElasticSearch of the retrieved examples per query\n",
            " |              - **total_indices** (`List[List[int]]`): the indices of the retrieved examples per query\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from TensorflowDatasetMixin:\n",
            " |  \n",
            " |  to_tf_dataset(self, batch_size: Optional[int] = None, columns: Union[str, List[str], NoneType] = None, shuffle: bool = False, collate_fn: Optional[Callable] = None, drop_remainder: bool = False, collate_fn_args: Optional[Dict[str, Any]] = None, label_cols: Union[str, List[str], NoneType] = None, prefetch: bool = True, num_workers: int = 0, num_test_batches: int = 20)\n",
            " |      Create a `tf.data.Dataset` from the underlying Dataset. This `tf.data.Dataset` will load and collate batches from\n",
            " |      the Dataset, and is suitable for passing to methods like `model.fit()` or `model.predict()`. The dataset will yield\n",
            " |      `dicts` for both inputs and labels unless the `dict` would contain only a single key, in which case a raw\n",
            " |      `tf.Tensor` is yielded instead.\n",
            " |      \n",
            " |      Args:\n",
            " |          batch_size (`int`, *optional*):\n",
            " |              Size of batches to load from the dataset. Defaults to `None`, which implies that the dataset won't be\n",
            " |              batched, but the returned dataset can be batched later with `tf_dataset.batch(batch_size)`.\n",
            " |          columns (`List[str]` or `str`, *optional*):\n",
            " |              Dataset column(s) to load in the `tf.data.Dataset`.\n",
            " |              Column names that are created by the `collate_fn` and that do not exist in the original dataset can be used.\n",
            " |          shuffle(`bool`, defaults to `False`):\n",
            " |              Shuffle the dataset order when loading. Recommended `True` for training, `False` for\n",
            " |              validation/evaluation.\n",
            " |          drop_remainder(`bool`, defaults to `False`):\n",
            " |              Drop the last incomplete batch when loading. Ensures\n",
            " |              that all batches yielded by the dataset will have the same length on the batch dimension.\n",
            " |          collate_fn(`Callable`, *optional*):\n",
            " |              A function or callable object (such as a `DataCollator`) that will collate\n",
            " |              lists of samples into a batch.\n",
            " |          collate_fn_args (`Dict`, *optional*):\n",
            " |              An optional `dict` of keyword arguments to be passed to the\n",
            " |              `collate_fn`.\n",
            " |          label_cols (`List[str]` or `str`, defaults to `None`):\n",
            " |              Dataset column(s) to load as labels.\n",
            " |              Note that many models compute loss internally rather than letting Keras do it, in which case\n",
            " |              passing the labels here is optional, as long as they're in the input `columns`.\n",
            " |          prefetch (`bool`, defaults to `True`):\n",
            " |              Whether to run the dataloader in a separate thread and maintain\n",
            " |              a small buffer of batches for training. Improves performance by allowing data to be loaded in the\n",
            " |              background while the model is training.\n",
            " |          num_workers (`int`, defaults to `0`):\n",
            " |              Number of workers to use for loading the dataset. Only supported on Python versions >= 3.8.\n",
            " |          num_test_batches (`int`, defaults to `20`):\n",
            " |              Number of batches to use to infer the output signature of the dataset.\n",
            " |              The higher this number, the more accurate the signature will be, but the longer it will take to\n",
            " |              create the dataset.\n",
            " |      \n",
            " |      Returns:\n",
            " |          `tf.data.Dataset`\n",
            " |      \n",
            " |      Example:\n",
            " |      \n",
            " |      ```py\n",
            " |      >>> ds_train = ds[\"train\"].to_tf_dataset(\n",
            " |      ...    columns=['input_ids', 'token_type_ids', 'attention_mask', 'label'],\n",
            " |      ...    shuffle=True,\n",
            " |      ...    batch_size=16,\n",
            " |      ...    collate_fn=data_collator,\n",
            " |      ... )\n",
            " |      ```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "log.debug(ds)\n",
        "\n",
        "# MemoryMappedTable \n",
        "log.debug(f\"type = {type(ds.data['train'])}\")\n",
        "\n",
        "# 학습 데이터의 레이블 분포 확인\n",
        "labels = [example['label'] for example in ds['train']]\n",
        "log.debug(\"Label Distribution:\", Counter(labels))\n",
        "\n",
        "# 데이터셋 구조를 확인합니다.\n",
        "log.debug(f\"{help(ds['train'])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'promptID': Value(dtype='int32', id=None),\n",
              " 'pairID': Value(dtype='string', id=None),\n",
              " 'premise': Value(dtype='string', id=None),\n",
              " 'premise_binary_parse': Value(dtype='string', id=None),\n",
              " 'premise_parse': Value(dtype='string', id=None),\n",
              " 'hypothesis': Value(dtype='string', id=None),\n",
              " 'hypothesis_binary_parse': Value(dtype='string', id=None),\n",
              " 'hypothesis_parse': Value(dtype='string', id=None),\n",
              " 'genre': Value(dtype='string', id=None),\n",
              " 'label': ClassLabel(names=['entailment', 'neutral', 'contradiction'], id=None)}"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ds['train'].features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def to_tokenizer_fn(text, max_length=128):\n",
        "    return tokenizer(text, padding=True, truncation=True, max_length=max_length)\n",
        "\n",
        "def to_long_tensor_fn(input_tokenizer):\n",
        "    return torch.LongTensor(input_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "  \n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    texts.append(f\"{row['premise']} [SEP] {row['hypothesis']}\")\n",
        "    labels.append(row['label'])\n",
        "  \n",
        "  token = to_long_tensor_fn(to_tokenizer_fn(texts, 128).input_ids)\n",
        "  attention_mask = to_long_tensor_fn(to_tokenizer_fn(texts).attention_mask)\n",
        "  label = to_long_tensor_fn(labels)\n",
        "  log.debug(token)\n",
        "  log.debug(attention_mask)\n",
        "  log.debug(label)\n",
        "  return token, attention_mask, label\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCDOE] 데이터로더를 생성\n",
        "\n",
        "- 학습데이터\n",
        "- 검증 데이터\n",
        "- 빠르게 학습하기 위한 데이터로더"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 200,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_balanced_subset(dataset, samples_per_label):\n",
        "    \"\"\"\n",
        "    각 레이블별로 균등하게 샘플링된 서브셋을 생성.\n",
        "    \n",
        "    Args:\n",
        "        dataset: 전체 데이터셋 (HuggingFace Dataset 형태)\n",
        "        samples_per_label: 각 레이블당 샘플링할 개수\n",
        "    \n",
        "    Returns:\n",
        "        균등하게 샘플링된 데이터셋 (Dataset 객체)\n",
        "    \"\"\"\n",
        "    from collections import defaultdict\n",
        "    import random\n",
        "\n",
        "    # 레이블별 데이터 분류\n",
        "    label_to_samples = defaultdict(list)\n",
        "    for example in dataset:\n",
        "        label_to_samples[example['label']].append(example)\n",
        "\n",
        "    # 각 레이블에서 필요한 개수만큼 샘플링\n",
        "    balanced_samples = []\n",
        "    for label, samples in label_to_samples.items():\n",
        "        if len(samples) < samples_per_label:\n",
        "            raise ValueError(f\"Not enough samples for label {label}.\")\n",
        "        balanced_samples.extend(random.sample(samples, samples_per_label))\n",
        "\n",
        "    return Dataset.from_list(balanced_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:root:Dataset({\n",
            "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "    num_rows: 392702\n",
            "})\n",
            "DEBUG:root:Dataset({\n",
            "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "    num_rows: 9815\n",
            "})\n",
            "DEBUG:root:Dataset({\n",
            "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "    num_rows: 1200\n",
            "})\n",
            "DEBUG:root:Dataset({\n",
            "    features: ['promptID', 'pairID', 'premise', 'premise_binary_parse', 'premise_parse', 'hypothesis', 'hypothesis_binary_parse', 'hypothesis_parse', 'genre', 'label'],\n",
            "    num_rows: 120\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# 데이터 학습 데이터셋\n",
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=128, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 데이터 검증 데이터셋 \n",
        "test_loader = DataLoader(\n",
        "    ds['validation_matched'], batch_size=128, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 빠른 모델 검증을 위한 작은 수의 데이터만을 학습\n",
        "train_loader_small = create_balanced_subset(ds['train'], 400)\n",
        "train_loader_small = DataLoader(\n",
        "    train_loader_small, batch_size=128, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "# 빠른 모델 검증을 위한 작은 수의 데이터만을 학습\n",
        "test_loader_small = create_balanced_subset(ds['validation_matched'], 40)\n",
        "test_loader_small = DataLoader(\n",
        "    test_loader_small, batch_size=128, shuffle=False, collate_fn=collate_fn\n",
        ")\n",
        "\n",
        "log.debug(train_loader.dataset)\n",
        "log.debug(test_loader.dataset)\n",
        "log.debug(train_loader_small.dataset)\n",
        "log.debug(test_loader_small.dataset)\n",
        "\n",
        "# 학습 데이터의 레이블 분포 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] 학습할 데이터의 정보를 확인합니다. \n",
        "\n",
        "- 두개의 문장을 하나로 합치는 작업을 선행 확인해보기 \n",
        "    - 두 문장을 구분하는 값이 SEP 102으로 분리 되었는지 확인해보았습니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] train_dataloaders 에 따라 loader 변경 기능 추가 \n",
        "- train_dataLoaders 할당하는 loader로 학습 변경하기 위해 선언 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dataloaders = train_loader_small\n",
        "test_dataloaders = test_loader_small\n",
        "log.debug(f\"test_loader = {train_dataloaders.dataset}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] callate_fn 변경으로 인한 데이터 검증 확인을 위해 데이터 확인 \n",
        "\n",
        "[FEEDBACK]\n",
        "- 데이터를 검증하는 작업을 눈으로 확인하게 되었는데 python 또는 AI 모델에서 데이터를 검증하기 위한 수단이 무엇이 있을까요 \n",
        "    - TDD 같은 개념의 방법이 있나요?\n",
        "    - assert 검증을 하는 방식인지 궁금합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 223,
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset_premise = train_dataloaders.dataset['premise']\n",
        "dataset_hypothesis = train_dataloaders.dataset[\"hypothesis\"]\n",
        "dataset_labels = train_dataloaders.dataset[\"label\"]\n",
        "\n",
        "# 데이터 구조확인\n",
        "log.debug(f\"dataset_premise =  {dataset_premise}\")\n",
        "log.debug(f\"dataset_hypothesis = {dataset_hypothesis}\")\n",
        "log.debug(f\"dataset_labels = {dataset_labels}\")\n",
        "\n",
        "# 토크나이저 102 값이 중간에 넣어졌는지 확인하기\n",
        "log.debug(f\"{to_tokenizer_fn(dataset_premise)}\")\n",
        "log.debug(f\"{to_tokenizer_fn(dataset_hypothesis)}\")\n",
        "log.debug(f\"{collate_fn(train_dataloaders.dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bF34XkoYIeEm"
      },
      "source": [
        "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJaUp2Vob0U-",
        "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/naseunghoo/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /distilbert-base-uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 175,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 225,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW7ETZQzzNp2",
        "outputId": "acae0d36-0b4a-4c7c-a0cd-5171e7158cf2"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "    self.classifier = nn.Linear(768, 3)\n",
        "\n",
        "  def forward(self, x, attention_mask): \n",
        "    x = self.encoder(input_ids=x, attention_mask=attention_mask)\n",
        "    return self.classifier(x.last_hidden_state[:, 0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 226,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU7BWEbgJeKm"
      },
      "source": [
        "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
        "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
        "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
        "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
        "\n",
        "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "def live_plot(train_accs, test_accs, title=\"Real-time Accuracy\", label1='train', label2='test'):\n",
        "    # 플롯 크기 설정\n",
        "    plt.figure(figsize=(10, 8))\n",
        "\n",
        "    # 1번째 서브플롯: Train_loss vs test\n",
        "    plt.subplot(2, 1, 1)\n",
        "    x = np.arange(len(train_accs))\n",
        "    plt.plot(x, train_accs, label='train', color='blue')  # 훈련 손실\n",
        "    plt.plot(x, test_accs, label='test', color='red', marker='o')  # 검증 손실\n",
        "    plt.title(\"Train_acc vs test_acc\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "\n",
        "    # 플롯 간격 조정 및 출력\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, attention_mask, labels = data\n",
        "    inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "    preds = model(inputs, attention_mask)\n",
        "    preds = torch.argmax(preds, dim=-1)\n",
        "    #preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_checkpoint(epoch, model, optimizer, best_loss, counter, train_losses, train_accs, test_accs, filepath):\n",
        "    \"\"\"\n",
        "    학습 상태를 저장하는 함수\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_loss': best_loss,\n",
        "        'counter': counter,\n",
        "        'train_losses': train_losses,\n",
        "        'train_accs': train_accs,\n",
        "        'test_accs': test_accs\n",
        "    }\n",
        "    torch.save(checkpoint, filepath)\n",
        "    log.info(f\"Checkpoint saved at {filepath}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_checkpoint(filepath, model, optimizer):\n",
        "    \"\"\"\n",
        "    저장된 학습 상태를 복구하는 함수\n",
        "    \"\"\"\n",
        "    checkpoint = torch.load(filepath)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    best_loss = checkpoint['best_loss']\n",
        "    counter = checkpoint['counter']\n",
        "    train_losses = checkpoint['train_losses']\n",
        "    train_accs = checkpoint['train_accs']\n",
        "    test_accs = checkpoint['test_accs']\n",
        "    log.info(f\"Checkpoint loaded from {filepath}\")\n",
        "    return epoch, best_loss, counter, train_losses, train_accs, test_accs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[MYCODE] 학습 정보가 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "3363b8ca-7695-493f-96a0-5aa6b52d1d60"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /Users/naseunghoo/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "100%|██████████| 10/10 [05:15<00:00, 31.60s/it]\n",
            "INFO:root:Epoch 1: avg improved to 1.826.\n",
            "INFO:root:Checkpoint saved at checkpoint_epoch_1.pth\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "from tqdm import tqdm\n",
        "\n",
        "model = TextClassifier()\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to(device)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 10\n",
        "\n",
        "best_loss = float('inf')\n",
        "patience = 3\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "test_accs = []\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "    total_loss = 0.0\n",
        "    total_samples = 0\n",
        "    model.train()\n",
        "    for data in tqdm(train_dataloaders):\n",
        "        model.zero_grad()\n",
        "        inputs, attention_mask, labels = data\n",
        "        inputs, attention_mask, labels = inputs.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        preds = model(inputs, attention_mask)\n",
        "        loss = loss_fn(preds, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * inputs.size(0)\n",
        "        total_samples += inputs.size(0)\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    train_losses.append(avg_loss)\n",
        "\n",
        "    # 손실 감소 확인\n",
        "    if avg_loss < best_loss:\n",
        "        best_loss = avg_loss\n",
        "        counter = 0\n",
        "        log.info(f\"Epoch {epoch + 1}: avg improved to {avg_loss:.3f}.\")\n",
        "    else:\n",
        "        counter += 1\n",
        "        log.info(f\"Epoch {epoch + 1}: Loss did not improve for {counter} consecutive epochs.\")\n",
        "\n",
        "    # 학습 중단 조건\n",
        "    if counter >= patience:\n",
        "        log.info(f\"Stopping early at epoch {epoch + 1}. Best Loss: {best_loss:.3f}\")\n",
        "        break\n",
        "\n",
        "    # 학습 상태 저장\n",
        "    if epoch % 5 == 0:\n",
        "        save_path = f\"checkpoint_epoch_{epoch + 1}.pth\"\n",
        "        save_checkpoint(epoch + 1, model, optimizer, best_loss, counter, train_losses, train_accs, test_accs, save_path)\n",
        "\n",
        "    # 모델 평가\n",
        "    model.eval()\n",
        "    train_acc = accuracy(model, train_dataloaders)\n",
        "    test_acc = accuracy(model, test_dataloaders)\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "\n",
        "    # 시각화와 로그\n",
        "    if epoch % 5 == 0:\n",
        "        live_plot(train_accs, test_accs)\n",
        "    log.info(f\"Epoch {epoch + 1} | Train Loss: {avg_loss:.3f} | Train Acc: {train_acc:.3f} | Test Acc: {test_acc:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
