{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DistilBERT fine-tuning으로 감정 분석 모델 학습하기\n",
        "\n",
        "이번 실습에서는 pre-trained된 DistilBERT를 불러와 이전 주차 실습에서 사용하던 감정 분석 문제에 적용합니다. 먼저 필요한 library들을 불러옵니다."
      ],
      "metadata": {
        "id": "sbgz49PvHhLt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1LqgujQUbv6X",
        "outputId": "f8faaad2-ac05-401b-c8f1-45ec07cbb71b",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.35.10)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2024.5.15)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.21.0)\n",
            "Requirement already satisfied: botocore<1.36.0,>=1.35.10 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.35.10)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.7.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.36.0,>=1.35.10->boto3) (2.8.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.2->datasets) (4.12.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.10->boto3) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install tqdm boto3 requests regex sentencepiece sacremoses datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "그 후, 우리가 사용하는 DistilBERT pre-training 때 사용한 tokenizer를 불러옵니다."
      ],
      "metadata": {
        "id": "6YP3FxG9IF7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lGiZUoPby6e",
        "outputId": "592b19ee-1892-4821-fcef-24fe40cc185f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DistilBERT의 tokenizer를 불러왔으면 이제 `collate_fn`과 data loader를 정의합니다. 이 과정은 이전 실습과 동일하게 다음과 같이 구현할 수 있습니다."
      ],
      "metadata": {
        "id": "Cvfl_uFLIMWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset(\"stanfordnlp/imdb\")\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "  max_len = 400\n",
        "  texts, labels = [], []\n",
        "  for row in batch:\n",
        "    labels.append(row['label'])\n",
        "    texts.append(row['text'])\n",
        "\n",
        "  texts = torch.LongTensor(tokenizer(texts, padding=True, truncation=True, max_length=max_len).input_ids)\n",
        "  labels = torch.LongTensor(labels)\n",
        "\n",
        "  return texts, labels\n",
        "\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    ds['train'], batch_size=64, shuffle=True, collate_fn=collate_fn\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    ds['test'], batch_size=64, shuffle=False, collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "rE-y8sY9HuwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "이제 pre-trained DistilBERT를 불러옵니다. 이번에는 PyTorch hub에서 제공하는 DistilBERT를 불러봅시다."
      ],
      "metadata": {
        "id": "bF34XkoYIeEm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HJaUp2Vob0U-",
        "outputId": "4cabca2b-06ce-480c-d52a-1381a955464b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DistilBertModel(\n",
              "  (embeddings): Embeddings(\n",
              "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "    (position_embeddings): Embedding(512, 768)\n",
              "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (layer): ModuleList(\n",
              "      (0-5): 6 x TransformerBlock(\n",
              "        (attention): MultiHeadSelfAttention(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "        (ffn): FFN(\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (activation): GELUActivation()\n",
              "        )\n",
              "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "model = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "출력 결과를 통해 우리는 DistilBERT의 architecture는 일반적인 Transformer와 동일한 것을 알 수 있습니다.\n",
        "Embedding layer로 시작해서 여러 layer의 Attention, FFN를 거칩니다.\n",
        "\n",
        "이제 DistilBERT를 거치고 난 `[CLS]` token의 representation을 가지고 text 분류를 하는 모델을 구현합시다."
      ],
      "metadata": {
        "id": "uh-tqY8WInQt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW7ETZQzzNp2",
        "outputId": "acae0d36-0b4a-4c7c-a0cd-5171e7158cf2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class TextClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder = torch.hub.load('huggingface/pytorch-transformers', 'model', 'distilbert-base-uncased')\n",
        "    self.classifier = nn.Linear(768, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)['last_hidden_state']\n",
        "    x = self.classifier(x[:, 0])\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = TextClassifier()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위와 같이 `TextClassifier`의 `encoder`를 불러온 DistilBERT, 그리고 `classifier`를 linear layer로 설정합니다.\n",
        "그리고 `forward` 함수에서 순차적으로 사용하여 예측 결과를 반환합니다.\n",
        "\n",
        "다음은 마지막 classifier layer를 제외한 나머지 부분을 freeze하는 코드를 구현합니다."
      ],
      "metadata": {
        "id": "_hFvSis0JLju"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyTciaPZ0KYo"
      },
      "outputs": [],
      "source": [
        "for param in model.encoder.parameters():\n",
        "  param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위의 코드는 `encoder`에 해당하는 parameter들의 `requires_grad`를 `False`로 설정하는 모습입니다.\n",
        "`requires_grad`를 `False`로 두는 경우, gradient 계산 및 업데이트가 이루어지지 않아 결과적으로 학습이 되지 않습니다.\n",
        "즉, 마지막 `classifier`에 해당하는 linear layer만 학습이 이루어집니다.\n",
        "이런 식으로 특정 부분들을 freeze하게 되면 효율적으로 학습을 할 수 있습니다.\n",
        "\n",
        "마지막으로 이전과 같은 코드를 사용하여 학습 결과를 확인해봅시다."
      ],
      "metadata": {
        "id": "hU7BWEbgJeKm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvvaAEwCznt-",
        "outputId": "3363b8ca-7695-493f-96a0-5aa6b52d1d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch   0 | Train Loss: 234.78025314211845\n",
            "Epoch   1 | Train Loss: 199.59758660197258\n",
            "Epoch   2 | Train Loss: 186.83845001459122\n",
            "Epoch   3 | Train Loss: 179.1590757369995\n",
            "Epoch   4 | Train Loss: 174.46909701824188\n",
            "Epoch   5 | Train Loss: 171.1713205575943\n",
            "Epoch   6 | Train Loss: 169.39869466423988\n",
            "Epoch   7 | Train Loss: 167.80235224962234\n",
            "Epoch   8 | Train Loss: 165.50148677825928\n",
            "Epoch   9 | Train Loss: 164.10345768928528\n"
          ]
        }
      ],
      "source": [
        "from torch.optim import Adam\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "lr = 0.001\n",
        "model = model.to('cuda')\n",
        "loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer = Adam(model.parameters(), lr=lr)\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(n_epochs):\n",
        "  total_loss = 0.\n",
        "  model.train()\n",
        "  for data in train_loader:\n",
        "    model.zero_grad()\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda').float()\n",
        "\n",
        "    preds = model(inputs)[..., 0]\n",
        "    loss = loss_fn(preds, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "  print(f\"Epoch {epoch:3d} | Train Loss: {total_loss}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DjphVwXL00E2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7526ec71-f015-4f26-8035-3091ed71869e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=========> Train acc: 0.837 | Test acc: 0.828\n"
          ]
        }
      ],
      "source": [
        "def accuracy(model, dataloader):\n",
        "  cnt = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in dataloader:\n",
        "    inputs, labels = data\n",
        "    inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "\n",
        "    preds = model(inputs)\n",
        "    # preds = torch.argmax(preds, dim=-1)\n",
        "    preds = (preds > 0).long()[..., 0]\n",
        "\n",
        "    cnt += labels.shape[0]\n",
        "    acc += (labels == preds).sum().item()\n",
        "\n",
        "  return acc / cnt\n",
        "\n",
        "\n",
        "with torch.no_grad():\n",
        "  model.eval()\n",
        "  train_acc = accuracy(model, train_loader)\n",
        "  test_acc = accuracy(model, test_loader)\n",
        "  print(f\"=========> Train acc: {train_acc:.3f} | Test acc: {test_acc:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loss가 잘 떨어지고, 이전에 우리가 구현한 Transformer보다 더 빨리 수렴하는 것을 알 수 있습니다."
      ],
      "metadata": {
        "id": "rfFUkEM1ZWeG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Foks5u95ZQ1_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}