{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 목표 \n",
    "이전 이미지 관련 실습에서는 사용자가 이미지를 업로드하면 하드 코딩 된 prompt와 함께 GPT로 보내 답변을 받는 챗봇을 구현했습니다. 이번에는 다음과 같이 기능을 확장하는 것이 과제의 목표입니다:\n",
    "\n",
    "- 여러 이미지를 입력으로 받기\n",
    "    - 기존에는 이미지 한 장만을 입력으로 받았다면 이번에는 다수의 사진을 입력 받을 수 있도록 만들어야 합니다.\n",
    "    - Streamlit의 `st.file_uploader` [문서](https://docs.streamlit.io/develop/api-reference/widgets/st.file_uploader)를 확인하여 기존 코드에서 여러 장의 사진을 받을 수 있도록 구현해봅시다.\n",
    "- 업로드 된 이미지들을 가지고 자유롭게 질의응답 할 수 있는 챗봇 구현\n",
    "    - 기존과 다르게 하드코딩 된 prompt가 아닌, 사용자로부터 질문을 입력받아 GPT에게 넘겨주는 챗봇을 구현하셔야 합니다.\n",
    "    - 그리고 사용자가 여러 번 질문을 입력해도 처음 주어진 이미지들로 답변할 수 있도록 구현하셔야 합니다(이 부분은 RAG 실습 코드를 참조).\n",
    "    - GPT에게 여러 개의 사진을 보내주는 부분은 [API 문서](https://platform.openai.com/docs/guides/vision)를 확인하여 구현하시면 됩니다.\n",
    "- 다음 이미지들과 질문에 대한 챗봇의 답변 생성\n",
    "    - 다음 주어진 이미지들과 질문을 실제로 구현한 챗봇에게 주어졌을 때 어떤 답변이 생성되는지 영상을 녹화하셔야 합니다:\n",
    "        - 이미지: 인터넷에서 강아지 사진과 고양이 사진 각각 1장씩 찾아 입력으로 쓰시면 됩니다.\n",
    "        - 질문 1: 주어진 두 사진의 공통점이 뭐야?\n",
    "        - 질문 2: 주어진 두 사진의 차이점이 뭐야?\n",
    "    - 질문 1, 2를 차례로 입력하시면 됩니다. 즉, 챗봇과 질의응답이 두 차례 이루어져야 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 설계\n",
    "- 이미지 업로드를 위해서 이미지를 최대 3장 까지 받습니다. \n",
    "- 이미지 업로드 후 이미지에 대해서 별도의 질의를 할 수 있도록 구성했습니다.\n",
    "- 기본적인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import base64\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "import faiss\n",
    "import torch\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 환경 변수 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# OpenAI 모델 초기화\n",
    "chat_model = ChatOpenAI(model=\"gpt-4o-mini\", openai_api_key=OPENAI_API_KEY)\n",
    "\n",
    "# GPU 지원 여부 확인 (Apple Silicon 최적화)\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# CLIP 모델 로드 (세션 유지하여 메모리 절약)\n",
    "if \"clip_model\" not in st.session_state:\n",
    "    st.session_state.clip_model = CLIPModel.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    ).to(device)\n",
    "    st.session_state.clip_processor = CLIPProcessor.from_pretrained(\n",
    "        \"openai/clip-vit-base-patch32\"\n",
    "    )\n",
    "\n",
    "clip_model = st.session_state.clip_model\n",
    "clip_processor = st.session_state.clip_processor\n",
    "\n",
    "# FAISS 벡터 저장소 초기화 (한 번만 실행)\n",
    "if \"vector_db\" not in st.session_state:\n",
    "    faiss_index = faiss.IndexFlatL2(512)  # 512차원 벡터 저장\n",
    "    st.session_state.vector_db = faiss_index\n",
    "    st.session_state.image_metadata = {}  # FAISS에 저장된 이미지 정보 (파일명 저장)\n",
    "\n",
    "# 채팅 메시지 저장소 초기화 (채팅 기록 유지)\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "\n",
    "# CLIP을 사용하여 이미지 벡터화\n",
    "def get_image_embedding(image: Image.Image) -> np.ndarray:\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_model.get_image_features(**inputs)\n",
    "    return image_features.cpu().numpy().flatten()\n",
    "\n",
    "\n",
    "# 이미지를 Base64 인코딩하는 함수\n",
    "def encode_image_to_base64(image_data):\n",
    "    return base64.b64encode(image_data).decode(\"utf-8\")\n",
    "\n",
    "\n",
    "# FAISS에서 중복 이미지 확인\n",
    "def find_duplicate_image(image_vector):\n",
    "    \"\"\"FAISS에서 중복된 이미지가 있는지 확인하고, 있으면 해당 인덱스를 반환\"\"\"\n",
    "    if st.session_state.vector_db.ntotal == 0:\n",
    "        return None  # 저장된 벡터가 없으면 중복 아님\n",
    "\n",
    "    image_vector = np.array(image_vector, dtype=np.float32).reshape(1, -1)\n",
    "    D, I = st.session_state.vector_db.search(image_vector, 1)  # 가장 가까운 벡터 검색\n",
    "\n",
    "    if D[0][0] < 0.001:  # 거리가 0에 가까우면 동일한 이미지로 판단\n",
    "        return I[0][0]  # 중복된 이미지의 인덱스 반환\n",
    "    return None\n",
    "\n",
    "\n",
    "# 이미지 업로드\n",
    "uploaded_files = st.file_uploader(\n",
    "    \"이미지를 업로드 해주세요 (여러 개 가능)\",\n",
    "    type=[\"jpg\", \"jpeg\", \"png\"],\n",
    "    accept_multiple_files=True,\n",
    ")\n",
    "\n",
    "if uploaded_files:\n",
    "    for file in uploaded_files:\n",
    "        image = Image.open(file)\n",
    "        buffer = BytesIO()\n",
    "        image.save(buffer, format=\"PNG\")\n",
    "\n",
    "        # CLIP 벡터 생성\n",
    "        image_vector = get_image_embedding(image)\n",
    "        image_vector = np.array(image_vector, dtype=np.float32).reshape(1, -1)\n",
    "\n",
    "        # 중복된 이미지인지 확인\n",
    "        duplicate_index = find_duplicate_image(image_vector)\n",
    "        if duplicate_index is not None:\n",
    "            st.info(\n",
    "                f\"{file.name} 이미지는 이미 업로드된 이미지와 동일합니다. 기존 이미지를 유지합니다.\"\n",
    "            )\n",
    "        else:\n",
    "            # FAISS에 벡터 추가\n",
    "            st.session_state.vector_db.add(image_vector)\n",
    "\n",
    "            # 이미지 정보를 저장 (Base64 인코딩 포함)\n",
    "            image_base64 = encode_image_to_base64(buffer.getvalue())\n",
    "            st.session_state.image_metadata[st.session_state.vector_db.ntotal - 1] = {\n",
    "                \"name\": file.name,\n",
    "                \"base64\": image_base64,\n",
    "            }\n",
    "\n",
    "        st.image(image, caption=f\"업로드된 이미지: {file.name}\", use_column_width=True)\n",
    "\n",
    "    st.success(\n",
    "        f\"{len(uploaded_files)}개의 이미지가 업로드되었습니다. 질문을 입력하세요!\"\n",
    "    )\n",
    "\n",
    "# 기존 대화 기록 표시 (채팅 기록 유지)\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# 사용자 질문 처리\n",
    "prompt = st.chat_input(\"이미지에 대해 질문을 입력하세요 (예: 공통점은 무엇인가요?)\")\n",
    "\n",
    "if prompt:\n",
    "    # 사용자 입력 저장 및 표시\n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(prompt)\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # 업로드된 모든 이미지 정보를 OpenAI에게 전달 (Base64 포함)\n",
    "    if len(st.session_state.image_metadata) > 0:\n",
    "        image_info = []\n",
    "        for i, idx in enumerate(st.session_state.image_metadata):\n",
    "            image_data = st.session_state.image_metadata[idx]\n",
    "            image_info.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpeg;base64,{image_data['base64']}\"\n",
    "                    },\n",
    "                }\n",
    "            )\n",
    "        image_text = \"\\n\".join(\n",
    "            [\n",
    "                f\"이미지 {i+1}: {st.session_state.image_metadata[idx]['name']}\"\n",
    "                for i, idx in enumerate(st.session_state.image_metadata)\n",
    "            ]\n",
    "        )\n",
    "    else:\n",
    "        image_info = []\n",
    "        image_text = \"현재 업로드된 이미지가 없습니다.\"\n",
    "\n",
    "    print(image_text)\n",
    "\n",
    "    # OpenAI 프롬프트 구성\n",
    "    message = HumanMessage(\n",
    "        content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": f\"사용자가 {len(st.session_state.image_metadata)} 개의 이미지를 업로드했습니다.\\n{image_text}\\n\\n사용자의 질문: {prompt}\\n\",\n",
    "            },\n",
    "        ]\n",
    "        + image_info,  # 이미지 리스트를 추가\n",
    "    )\n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        st.markdown(\"응답 생성 중...\")\n",
    "\n",
    "        # OpenAI API 호출\n",
    "        result = chat_model.invoke([message])\n",
    "        response = result.content.strip()\n",
    "\n",
    "        # AI 응답 저장 및 표시\n",
    "        st.markdown(response)\n",
    "        st.session_state.messages.append({\"role\": \"assistant\", \"content\": response})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hanghae-subject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
